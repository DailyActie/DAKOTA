\chapter{Optimization Under Uncertainty (OUU)}\label{ouu}

\section{Reliability-Based Design Optimization (RBDO)}\label{ouu:rbdo}

Reliability-based design optimization (RBDO) methods are used to
perform design optimization accounting for reliability metrics.  The
reliability analysis capabilities described in
Section~\ref{uq:reliability:local} provide a substantial foundation
for exploring a variety of gradient-based RBDO formulations.
\cite{firstorder} investigated bi-level, fully-analytic bi-level, and
first-order sequential RBDO approaches employing underlying
first-order reliability assessments.  \cite{secondorder} investigated
fully-analytic bi-level and second-order sequential RBDO approaches
employing underlying second-order reliability assessments.  These
methods are overviewed in the following sections.

\subsection{Bi-level RBDO} \label{ouu:rbdo:bilev}

The simplest and most direct RBDO approach is the bi-level approach in
which a full reliability analysis is performed for every optimization
function evaluation.  This involves a nesting of two distinct levels
of optimization within each other, one at the design level and one at
the MPP search level.

Since an RBDO problem will typically specify both the $\bar{z}$ level
and the $\bar{p}/\bar{\beta}$ level, one can use either the RIA or the
PMA formulation for the UQ portion and then constrain the result in
the design optimization portion.  In particular, RIA reliability
analysis maps $\bar{z}$ to $p/\beta$, so RIA RBDO constrains $p/\beta$:
\begin{eqnarray}
  {\rm minimize }     & f \nonumber \\
  {\rm subject \ to } & \beta \ge \bar{\beta} \nonumber \\
  {\rm or }           & p \le \bar{p} \label{eq:rbdo_ria}
\end{eqnarray}

\noindent And PMA reliability analysis maps $\bar{p}/\bar{\beta}$ to 
$z$, so PMA RBDO constrains $z$:
\begin{eqnarray}
  {\rm minimize }     & f \nonumber \\
  {\rm subject \ to } & z \ge \bar{z} \label{eq:rbdo_pma}
\end{eqnarray}

\noindent where $z \ge \bar{z}$ is used as the RBDO constraint for 
a cumulative failure probability (failure defined as $z \le \bar{z}$)
but $z \le \bar{z}$ would be used as the RBDO constraint for a
complementary cumulative failure probability (failure defined as $z
\ge \bar{z}$).  It is worth noting that DAKOTA is not limited to these
types of inequality-constrained RBDO formulations; rather, they are
convenient examples.  DAKOTA supports general optimization under
uncertainty mappings~\cite{Eld02} which allow flexible use of
statistics within multiple objectives, inequality constraints, and
equality constraints.

An important performance enhancement for bi-level methods is the use
of sensitivity analysis to analytically compute the design gradients
of probability, reliability, and response levels.  When design
variables are separate from the uncertain variables (i.e., they are
not distribution parameters), then the following first-order 
expressions may be used~\cite{Hoh86,Kar92,All04}:
\begin{eqnarray}
\nabla_{\bf d} z           & = & \nabla_{\bf d} g \label{eq:deriv_z} \\
\nabla_{\bf d} \beta_{cdf} & = & \frac{1}{{\parallel \nabla_{\bf u} G 
\parallel}} \nabla_{\bf d} g \label{eq:deriv_beta} \\
\nabla_{\bf d} p_{cdf}     & = & -\phi(-\beta_{cdf}) \nabla_{\bf d} \beta_{cdf}
\label{eq:deriv_p}
\end{eqnarray}
where it is evident from Eqs.~\ref{eq:beta_cdf_ccdf}-\ref{eq:p_cdf_ccdf} 
that $\nabla_{\bf d} \beta_{ccdf} = -\nabla_{\bf d} \beta_{cdf}$ and 
$\nabla_{\bf d} p_{ccdf} = -\nabla_{\bf d} p_{cdf}$.  In the case of 
second-order integrations, Eq.~\ref{eq:deriv_p} must be expanded to 
include the curvature correction.  For Breitung's correction 
(Eq.~\ref{eq:p_2nd_breit}),
\begin{equation}
\nabla_{\bf d} p_{cdf} = \left[ \Phi(-\beta_p) \sum_{i=1}^{n-1} 
\left( \frac{-\kappa_i}{2 (1 + \beta_p \kappa_i)^{\frac{3}{2}}}
\prod_{\stackrel{\scriptstyle j=1}{j \ne i}}^{n-1} 
\frac{1}{\sqrt{1 + \beta_p \kappa_j}} \right) - 
\phi(-\beta_p) \prod_{i=1}^{n-1} \frac{1}{\sqrt{1 + \beta_p \kappa_i}} 
\right] \nabla_{\bf d} \beta_{cdf} \label{eq:deriv_p_breit}
\end{equation}
where $\nabla_{\bf d} \kappa_i$ has been neglected and $\beta_p \ge 0$
(see Section~\ref{uq:reliability:mpp:int}).  Other approaches assume
the curvature correction is nearly independent of the design
variables~\cite{Rac02}, which is equivalent to neglecting the first
term in Eq.~\ref{eq:deriv_p_breit}.

To capture second-order probability estimates within an RIA RBDO
formulation using well-behaved $\beta$ constraints, a generalized 
reliability index can be introduced where, similar to Eq.~\ref{eq:beta_cdf},
\begin{equation}
\beta^*_{cdf} = -\Phi^{-1}(p_{cdf}) \label{eq:gen_beta}
\end{equation}
for second-order $p_{cdf}$.  This reliability index is no longer
equivalent to the magnitude of ${\bf u}$, but rather is a convenience
metric for capturing the effect of more accurate probability
estimates.  The corresponding generalized reliability index
sensitivity, similar to Eq.~\ref{eq:deriv_p}, is
\begin{equation}
\nabla_{\bf d} \beta^*_{cdf} = -\frac{1}{\phi(-\beta^*_{cdf})}
\nabla_{\bf d} p_{cdf} \label{eq:deriv_gen_beta}
\end{equation}
where $\nabla_{\bf d} p_{cdf}$ is defined from Eq.~\ref{eq:deriv_p_breit}.
Even when $\nabla_{\bf d} g$ is estimated numerically,
Eqs.~\ref{eq:deriv_z}-\ref{eq:deriv_gen_beta} can be used to avoid
numerical differencing across full reliability analyses.

When the design variables are distribution parameters of the uncertain
variables, $\nabla_{\bf d} g$ is expanded with the chain rule and
Eqs.~\ref{eq:deriv_z} and~\ref{eq:deriv_beta} become
\begin{eqnarray}
\nabla_{\bf d} z           & = & \nabla_{\bf d} {\bf x} \nabla_{\bf x} g
\label{eq:deriv_z_ds} \\
\nabla_{\bf d} \beta_{cdf} & = & \frac{1}{{\parallel \nabla_{\bf u} G 
\parallel}} \nabla_{\bf d} {\bf x} \nabla_{\bf x} g \label{eq:deriv_beta_ds}
\end{eqnarray}
where the design Jacobian of the transformation ($\nabla_{\bf d} {\bf x}$)
may be obtained analytically for uncorrelated ${\bf x}$ or 
semi-analytically for correlated ${\bf x}$ ($\nabla_{\bf d} {\bf L}$
is evaluated numerically) by differentiating Eqs.~\ref{eq:trans_zx} 
and~\ref{eq:trans_zu} with respect to the distribution parameters.
Eqs.~\ref{eq:deriv_p}-\ref{eq:deriv_gen_beta} remain the same as
before.  For this design variable case, all required information for 
the sensitivities is available from the MPP search.

Since Eqs.~\ref{eq:deriv_z}-\ref{eq:deriv_beta_ds} are derived using
the Karush-Kuhn-Tucker optimality conditions for a converged MPP, they
are appropriate for RBDO using AMV+, AMV$^2$+, TANA, FORM, and SORM,
but not for RBDO using MVFOSM, MVSOSM, AMV, or AMV$^2$.


\subsection{Sequential/Surrogate-based RBDO} \label{ouu:rbdo:surr}

An alternative RBDO approach is the sequential approach, in which
additional efficiency is sought through breaking the nested
relationship of the MPP and design searches.  The general concept is
to iterate between optimization and uncertainty quantification,
updating the optimization goals based on the most recent probabilistic
assessment results.  This update may be based on safety
factors~\cite{Wu01} or other approximations~\cite{Du04}.

A particularly effective approach for updating the optimization goals
is to use the $p/\beta/z$ sensitivity analysis of
Eqs.~\ref{eq:deriv_z}-\ref{eq:deriv_beta_ds} in combination with local
surrogate models~\cite{Zou04}.  In \cite{Eld05} and~\cite{Eld06a},
first-order and second-order Taylor series approximations were
employed within a trust-region model management framework~\cite{Giu00}
in order to adaptively manage the extent of the approximations and
ensure convergence of the RBDO process.  Surrogate models were used
for both the objective function and the constraints, although the use
of constraint surrogates alone is sufficient to remove the nesting.

In particular, RIA trust-region surrogate-based RBDO employs surrogate
models of $f$ and $p/\beta$ within a trust region $\Delta^k$ centered
at ${\bf d}_c$.  For first-order surrogates:
\begin{eqnarray}
  {\rm minimize }     & f({\bf d}_c) + \nabla_d f({\bf d}_c)^T
({\bf d} - {\bf d}_c) \nonumber \\
  {\rm subject \ to } & \beta({\bf d}_c) + \nabla_d \beta({\bf d}_c)^T
({\bf d} - {\bf d}_c) \ge \bar{\beta} \nonumber \\
  {\rm or }           & p ({\bf d}_c) + \nabla_d p({\bf d}_c)^T 
({\bf d} - {\bf d}_c) \le \bar{p} \nonumber \\
& {\parallel {\bf d} - {\bf d}_c \parallel}_\infty \le \Delta^k
\label{eq:rbdo_surr1_ria}
\end{eqnarray}
and for second-order surrogates:
\begin{eqnarray}
  {\rm minimize }     & f({\bf d}_c) + \nabla_{\bf d} f({\bf d}_c)^T
({\bf d} - {\bf d}_c)  + \frac{1}{2} ({\bf d} - {\bf d}_c)^T 
\nabla^2_{\bf d} f({\bf d}_c) ({\bf d} - {\bf d}_c) \nonumber \\
  {\rm subject \ to } & \beta({\bf d}_c) + \nabla_{\bf d} \beta({\bf d}_c)^T
({\bf d} - {\bf d}_c) + \frac{1}{2} ({\bf d} - {\bf d}_c)^T 
\nabla^2_{\bf d} \beta({\bf d}_c) ({\bf d} - {\bf d}_c) \ge \bar{\beta}
\nonumber \\
  {\rm or }           & p ({\bf d}_c) + \nabla_{\bf d} p({\bf d}_c)^T 
({\bf d} - {\bf d}_c) + \frac{1}{2} ({\bf d} - {\bf d}_c)^T 
\nabla^2_{\bf d} p({\bf d}_c) ({\bf d} - {\bf d}_c) \le \bar{p} \nonumber \\
& {\parallel {\bf d} - {\bf d}_c \parallel}_\infty \le \Delta^k
\label{eq:rbdo_surr2_ria}
\end{eqnarray}
For PMA trust-region surrogate-based RBDO, surrogate models of
$f$ and $z$ are employed within a trust region $\Delta^k$ centered 
at ${\bf d}_c$.  For first-order surrogates:
\begin{eqnarray}
  {\rm minimize }     & f({\bf d}_c) + \nabla_d f({\bf d}_c)^T
({\bf d} - {\bf d}_c) \nonumber \\
  {\rm subject \ to } & z({\bf d}_c) + \nabla_d z({\bf d}_c)^T ({\bf d} - {\bf d}_c) 
\ge \bar{z} \nonumber \\
& {\parallel {\bf d} - {\bf d}_c \parallel}_\infty \le \Delta^k
\label{eq:rbdo_surr1_pma}
\end{eqnarray}
and for second-order surrogates:
\begin{eqnarray}
  {\rm minimize }     & f({\bf d}_c) + \nabla_{\bf d} f({\bf d}_c)^T
({\bf d} - {\bf d}_c) + \frac{1}{2} ({\bf d} - {\bf d}_c)^T 
\nabla^2_{\bf d} f({\bf d}_c) ({\bf d} - {\bf d}_c) \nonumber \\
  {\rm subject \ to } & z({\bf d}_c) + \nabla_{\bf d} z({\bf d}_c)^T ({\bf d} - {\bf d}_c)
 + \frac{1}{2} ({\bf d} - {\bf d}_c)^T \nabla^2_{\bf d} z({\bf d}_c) 
({\bf d} - {\bf d}_c) \ge \bar{z} \nonumber \\
& {\parallel {\bf d} - {\bf d}_c \parallel}_\infty \le \Delta^k
\label{eq:rbdo_surr2_pma}
\end{eqnarray}
where the sense of the $z$ constraint may vary as described
previously.  The second-order information in
Eqs.~\ref{eq:rbdo_surr2_ria} and \ref{eq:rbdo_surr2_pma} will
typically be approximated with quasi-Newton updates.


\section{Stochastic Expansion-Based Design Optimization (SEBDO)} \label{ouu:sebdo}


Section~\ref{uq:expansion:rvsa} describes sensitivity analysis of the
polynomial chaos expansion with respect to the random variables.  Here
we extend this analysis to include sensitivity analysis with respect
to design variables.  With the introduction of design variables
$\boldsymbol{s}$, a polynomial chaos expansion only over the random
variables $\boldsymbol{\xi}$ has the functional relationship:
\begin{equation}
R(\boldsymbol{\xi}, \boldsymbol{s}) \cong \sum_{j=0}^P \alpha_j(\boldsymbol{s}) 
\Psi_j(\boldsymbol{\xi}) \label{eq:R_alpha_s_psi_xi}
\end{equation}

\noindent In this case, design sensitivities for the mean and variance 
in Eqs.~\ref{eq:mean_pce} and~\ref{eq:covar_pce} are as follows:
\begin{eqnarray}
\frac{d\mu_R}{ds} &=& \frac{d\alpha_0}{ds} 
~~=~~ \frac{d}{ds} \langle R \rangle 
~~=~~ \langle \frac{dR}{ds} \rangle \label{eq:dmuR_ds_unc_pce} \\
\frac{d\sigma^2_R}{ds} &=& \sum_{j=1}^P \langle \Psi_j^2 \rangle 
\frac{d\alpha_j^2}{ds}
~~=~~ 2 \sum_{j=1}^P \alpha_j \langle \frac{dR}{ds}, \Psi_j \rangle 
\label{eq:dsigR_ds_unc_pce}
%2 \sigma_R \frac{d\sigma_R}{ds} &=& 2 
%\sum_{j=1}^P \alpha_j \frac{d\alpha_j}{ds} \langle \Psi_j^2 \rangle \\
%\frac{d\sigma_R}{ds} &=& \frac{1}{\sigma_R} 
%\sum_{j=1}^P \alpha_j \frac{d}{ds} \langle R, \Psi_j \rangle 
%\label{eq:dsigR_ds_unc_pce}
\end{eqnarray}
since
\begin{equation}
\frac{d\alpha_j}{ds} = \frac{\langle \frac{dR}{ds}, \Psi_j \rangle}
{\langle \Psi^2_j \rangle} \label{eq:dalpha_j_ds}
\end{equation}
The coefficients calculated in Eq.~\ref{eq:dalpha_j_ds} may be
interpreted as either the design sensitivities of the chaos
coefficients for the response expansion or the chaos coefficients of
an expansion for the response design sensitivities.  The evaluation of
integrals involving $\frac{dR}{ds}$ extends the data requirements for
the PCE approach to include response sensitivities at each of the
sampling points for the quadrature, sparse grid, sampling, or point
collocation coefficient estimation approaches.  The resulting
expansions are valid only for a particular set of design variables and
must be recalculated each time the design variables are modified.

Similarly for stochastic collocation,
\begin{equation}
R(\boldsymbol{\xi}, \boldsymbol{s}) \cong \sum_{j=1}^{N_p} r_j(\boldsymbol{s}) 
\boldsymbol{L}_j(\boldsymbol{\xi}) \label{eq:R_r_s_L_xi}
\end{equation}
leads to
\begin{eqnarray}
\frac{d\mu_R}{ds} &=& \frac{d}{ds} \langle R \rangle 
~~=~~ \sum_{j=1}^{N_p} \frac{dr_j}{ds} \langle \boldsymbol{L}_j \rangle 
~~=~~ \sum_{j=1}^{N_p} w_j \frac{dr_j}{ds} \label{eq:dmuR_ds_xi_sc} \\
\frac{d\sigma^2_R}{ds} &=& \sum_{j=1}^{N_p} 2 w_j r_j \frac{dr_j}{ds}
- 2 \mu_R \frac{d\mu_R}{ds} 
~~=~~ \sum_{j=1}^{N_p} 2 w_j (r_j - \mu_R) \frac{dr_j}{ds}
\label{eq:dsigR_ds_xi_sc}
\end{eqnarray}
based on differentiation of Eqs.~\ref{eq:mean_sc}-\ref{eq:covar_sc}.

Alternatively, a stochastic expansion can be formed over both
$\boldsymbol{\xi}$ and $\boldsymbol{s}$.  Assuming a bounded
design domain $\boldsymbol{s}_L \le \boldsymbol{s} \le
\boldsymbol{s}_U$ (with no implied probability content), a Legendre 
chaos basis would be appropriate for each of the dimensions in 
$\boldsymbol{s}$ within a polynomial chaos expansion.
\begin{equation}
R(\boldsymbol{\xi}, \boldsymbol{s}) \cong \sum_{j=0}^P \alpha_j 
\Psi_j(\boldsymbol{\xi}, \boldsymbol{s}) \label{eq:R_alpha_psi_xi_s}
\end{equation}

\noindent In this case, design sensitivities for the mean and variance 
do not require response sensitivity data, but this comes at the cost
of forming the PCE over additional dimensions.  For this combined
variable expansion, the mean and variance are evaluated by evaluating
the expectations over only the random variables, which eliminates the
polynomial dependence on $\boldsymbol{\xi}$, leaving behind the
desired polynomial dependence on $\boldsymbol{s}$:
\begin{eqnarray}
\mu_R(\boldsymbol{s}) &=& \sum_{j=0}^P \alpha_j \langle \Psi_j(\boldsymbol{\xi},
\boldsymbol{s}) \rangle_{\boldsymbol{\xi}} \label{eq:muR_comb_pce} \\
\sigma^2_R(\boldsymbol{s}) &=& \sum_{j=0}^P \sum_{k=0}^P \alpha_j \alpha_k 
\langle \Psi_j(\boldsymbol{\xi}, \boldsymbol{s}) \Psi_k(\boldsymbol{\xi},
\boldsymbol{s}) \rangle_{\boldsymbol{\xi}} ~-~ \mu^2_R(\boldsymbol{s})
\label{eq:sigR_comb_pce}
\end{eqnarray}
The remaining polynomials may then be differentiated with respect to
$\boldsymbol{s}$. % as in Eqs.~\ref{eq:dR_dxi_pce}-\ref{eq:deriv_prod_pce}.
In this approach, the combined PCE is valid for the full design
variable range ($\boldsymbol{s}_L \le \boldsymbol{s} \le \boldsymbol{s}_U$)
and does not need to be updated for each change in design variables,
although adaptive localization techniques (i.e., trust region model
management approaches) can be employed when improved local accuracy of
the sensitivities is required.
% Q: how is TR ratio formed if exact soln can't be evaluated?
% A: if objective is accuracy over a design range, then truth is PCE/SC
%    at a single design point!  -->>  Can use first-order corrections based
%    on the 2 different SSA approaches!  This is a multifidelity SBO using
%    HF = Uncertain expansion, LF = Combined expansion. Should get data reuse.

Similarly for stochastic collocation,
\begin{equation}
R(\boldsymbol{\xi}, \boldsymbol{s}) \cong \sum_{j=1}^{N_p} r_j 
\boldsymbol{L}_j(\boldsymbol{\xi}, \boldsymbol{s}) \label{eq:R_r_L_xi_s}
\end{equation}
leads to
\begin{eqnarray}
\mu_R(\boldsymbol{s}) &=& \sum_{j=1}^{N_p} r_j \langle 
\boldsymbol{L}_j(\boldsymbol{\xi}, \boldsymbol{s}) \rangle_{\boldsymbol{\xi}} 
\label{eq:muR_both_sc} \\
\sigma^2_R(\boldsymbol{s}) &=& \sum_{j=1}^{N_p} \sum_{k=1}^{N_p} r_j r_k 
\langle \boldsymbol{L}_j(\boldsymbol{\xi}, \boldsymbol{s}) 
\boldsymbol{L}_k(\boldsymbol{\xi}, \boldsymbol{s}) \rangle_{\boldsymbol{\xi}}
~-~ \mu^2_R(\boldsymbol{s}) \label{eq:sigR_both_sc}
\end{eqnarray}
where the remaining polynomials not eliminated by the expectation over
$\boldsymbol{\xi}$ are again differentiated with respect to $\boldsymbol{s}$.

Given the capability to compute analytic statistics of the response
along with design sensitivities of these statistics, DAKOTA supports
bi-level, sequential, and multifidelity approaches for optimization
under uncertainty (OUU). %for reliability-based design and robust design.
The latter two approaches apply surrogate modeling approaches (data 
fits and multifidelity modeling) to the uncertainty analysis and then 
apply trust region model management to the optimization process.

The simplest and most direct approach is to employ these analytic
statistics and their design derivatives directly within an
optimization loop.  This approach is known as bi-level OUU, since
there is an inner level uncertainty analysis nested within an outer
level optimization.

Consider the common reliability-based design example of a deterministic 
objective function with a reliability constraint:
\begin{eqnarray}
  {\rm minimize }     & f \nonumber \\
  {\rm subject \ to } & \beta \ge \bar{\beta} \label{eq:rbdo}
\end{eqnarray}
where $\beta$ is computed relative to a prescribed threshold response
value $\bar{z}$ and is constrained by a prescribed reliability level
$\bar{\beta}$.  Another common example is robust design in which the
constraint enforcing a reliability lower-bound has been replaced with
a constraint enforcing a variance upper-bound:
\begin{eqnarray}
  {\rm minimize }     & f \nonumber \\
  {\rm subject \ to } & \sigma^2 \le \bar{\sigma}^2 \label{eq:rdo}
\end{eqnarray}
%It is worth noting that DAKOTA is not limited to these
%types of inequality-constrained OUU formulations; rather, they are
%convenient examples.  DAKOTA supports general optimization under
%uncertainty mappings~\cite{sbouu} which allow flexible use of
%statistics within multiple objectives, inequality constraints, and
%equality constraints.

\subsection{Bi-level SEBDO} \label{ouu:sebdo:bilev}

Solving these problems using a bi-level approach involves computing
$\beta(\boldsymbol{s})$ and $\frac{d\beta}{d\boldsymbol{s}}$ for
Eq.~\ref{eq:rbdo} or $\sigma^2$ and $\frac{d\sigma^2}{d\boldsymbol{s}}$
for Eq.~\ref{eq:rdo} for each set of design variables $\boldsymbol{s}$
passed from the optimizer.  This approach is supported for both 
Uncertain and Combined expansions using PCE and SC.

\subsection{Sequential/Surrogate-Based SEBDO} \label{ouu:sebdo:surr}

An alternative OUU approach is the sequential approach, in which
additional efficiency is sought through breaking the nested
relationship of the UQ and optimization loops.  The general concept is
to iterate between optimization and uncertainty quantification,
updating the optimization goals based on the most recent uncertainty
assessment results.  This approach is common with the reliability
methods community, for which the updating strategy may be based on
safety factors~\cite{Wu01} or other approximations~\cite{Du04}.

A particularly effective approach for updating the optimization goals
is to use data fit surrogate models, and in particular, local Taylor
series models allow direct insertion of stochastic sensitivity
analysis capabilities.  In Ref.~\cite{Eld05}, first-order Taylor
series approximations were explored, and in Ref.~\cite{Eld06a},
second-order Taylor series approximations are investigated.  In both
cases, a trust-region model management framework~\cite{Eld06b} is
used to adaptively manage the extent of the approximations and ensure
convergence of the OUU process.  Surrogate models are used for both
the objective and the constraint functions, although the use of
surrogates is only required for the functions containing statistical
results; deterministic functions may remain explicit is desired.

In particular, trust-region surrogate-based optimization for
reliability-based design employs surrogate models of $f$ and $\beta$
within a trust region $\Delta^k$ centered at ${\bf s}_c$:
\begin{eqnarray}
  {\rm minimize }     & f({\bf s}_c) + \nabla_s f({\bf s}_c)^T
({\bf s} - {\bf s}_c) \nonumber \\
  {\rm subject \ to } & \beta({\bf s}_c) + \nabla_s \beta({\bf s}_c)^T
({\bf s} - {\bf s}_c) \ge \bar{\beta} \\
& {\parallel {\bf s} - {\bf s}_c \parallel}_\infty \le \Delta^k \nonumber
\label{eq:rbdo_surr}
\end{eqnarray}
and trust-region surrogate-based optimization for robust design
employs surrogate models of $f$ and $\sigma^2$ within a trust region
$\Delta^k$ centered at ${\bf s}_c$:
\begin{eqnarray}
  {\rm minimize }     & f({\bf s}_c) + \nabla_s f({\bf s}_c)^T
({\bf s} - {\bf s}_c) \nonumber \\
  {\rm subject \ to } & \sigma^2({\bf s}_c) + \nabla_s \sigma^2({\bf s}_c)^T 
({\bf s} - {\bf s}_c) \le \bar{\sigma}^2 \\
& {\parallel {\bf s} - {\bf s}_c \parallel}_\infty \le \Delta^k \nonumber
\label{eq:rdo_surr}
\end{eqnarray}
Second-order local surrogates may also be employed, where the Hessians
are typically approximated with quasi-Newton updates.  The sequential
approach is available for Uncertain expansions using PCE and SC.

\subsection{Multifidelity SEBDO} \label{ouu:sebdo:mf}

The multifidelity OUU approach is another trust-region surrogate-based
approach.  Instead of the surrogate UQ model being a simple data fit
(in particular, first-/second-order Taylor series model) of the truth
UQ model results, we now employ distinct UQ models of differing
fidelity.  This differing UQ fidelity could stem from the fidelity of
the underlying simulation model, the fidelity of the UQ algorithm, or
both.  In this paper, we focus on the fidelity of the UQ algorithm.
For reliability methods, this could entail varying fidelity in
approximating assumptions (e.g., Mean Value for low fidelity, SORM for
high fidelity), and for stochastic expansion methods, it could involve
differences in selected levels of $p$ and $k$ refinement.

%Here we will explore multifidelity stochastic models and employ
%first-order additive corrections, where the meaning of multiple
%fidelities is expanded to imply the quality of multiple UQ analyses,
%not necessarily the fidelity of the underlying simulation model.  For
%example, taking an example from the reliability method family, one
%might employ the simple Mean Value method as a ``low fidelity'' UQ
%model and take SORM as a ``high fidelity'' UQ model.  In this case,
%the models do not differ in their ability to span a range of design
%parameters; rather, they differ in their sets of approximating
%assumptions about the characteristics of the response function.

Here, we define UQ fidelity as point-wise accuracy in the
design space and take the low fidelity model, whose validity over the
design space will be adaptively controlled, to be the Combined
expansion PCE/SC model, and the high fidelity truth model to be the
Uncertain expansion PCE/SC model, with validity only at a single
design point.  This will allow us to take advantage of the design
space spanning and lower cost of the Combined expansion approach to
the extent possible, with fallback to the greater accuracy and higher
expense of the Uncertain expansion approach when needed.  The Combined
expansion approach will span only the current trust region of the
design space and will need to be reconstructed for each new trust region.  
%While conceptually different, in the end, this approach is
%similar to the use of a global data fit surrogate-based optimization
%at the top level in combination with the Uncertain expansion PCE/SC at
%the lower level, with the distinction that the multifidelity approach
%embeds the design space spanning within a modified PCE/SC process
%whereas the data fit approach performs the design space spanning
%outside of the UQ (using data from a single unmodified PCE/SC process,
%which may now remain zeroth-order).
The design derivatives of each model provide the necessary data
to correct the low fidelity model to first-order consistency with the
high fidelity model at the center of each trust region.

Multifidelity optimization for reliability-based design can be
formulated as:
\begin{eqnarray}
  {\rm minimize }     & f({\bf s}) \nonumber \\
  {\rm subject \ to } & \hat{\beta_{hi}}({\bf s}) \ge \bar{\beta} \\
& {\parallel {\bf s} - {\bf s}_c \parallel}_\infty \le \Delta^k \nonumber
\label{eq:rbdo_mf}
\end{eqnarray}
and multifidelity optimization for robust design can be formulated as:
\begin{eqnarray}
  {\rm minimize }     & f({\bf s}) \nonumber \\
  {\rm subject \ to } & \hat{\sigma_{hi}}^2({\bf s}) \le \bar{\sigma}^2 \\
& {\parallel {\bf s} - {\bf s}_c \parallel}_\infty \le \Delta^k \nonumber
\label{eq:rdo_mf}
\end{eqnarray}
where the deterministic objective function is not approximated and 
$\hat{\beta_{hi}}$ and $\hat{\sigma_{hi}}^2$ are the approximated
high-fidelity UQ results resulting from correction of the low-fidelity 
UQ results.  In the case of an additive correction function:
\begin{eqnarray}
\hat{\beta_{hi}}({\bf s})    &=& \beta_{lo}({\bf s}) + 
\alpha_{\beta}({\bf s})  \label{eq:corr_lf_beta} \\
\hat{\sigma_{hi}}^2({\bf s}) &=& \sigma_{lo}^2({\bf s}) + 
\alpha_{\sigma^2}({\bf s}) \label{eq:corr_lf_sigma}
\end{eqnarray}
where correction functions $\alpha({\bf s})$ enforcing first-order
%and quasi-second-order 
consistency~\cite{Eld04} are typically employed.
