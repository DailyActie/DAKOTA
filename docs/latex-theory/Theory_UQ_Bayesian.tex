\chapter{Bayesian Methods}\label{uq:bayes}


This chapter covers theoretical aspects of methods for performing
Bayesian inference.

\section{Fundamentals} \label{uq:bayes:basic}

Bayes Theorem~\cite{Jaynes}, shown in Equation~\ref{eqn:BayesThm}, is
used for inference: to derive the plausible parameter values, based on
the prior probability density and the data $d$. The result is the
posterior parameter density of the parameters $f_{\Theta |D}\left(
{\theta |d} \right)$.
\begin{equation}
  \label{eqn:BayesThm}
  {f_{\Theta |D}}\left( {\theta |d} \right) = \frac{{{f_\Theta }\left( \theta  \right)\mathcal{L}\left( {\theta ;d} \right)}}{{{f_D}\left( d \right)}}
\end{equation}

The likelihood function is used to describe how well a model's
predictions are supported by the data.  
%The likelihood function can be written generally as:
%\begin{equation*}
%  \mathcal{L}\left( {\theta ;d} \right) = f\left( {\mathcal{M}\left( \theta  \right) - d} \right)
%\end{equation*}
The specific likelihood function currently used in Dakota is a Gaussian
likelihood. This means that we assume the difference between the model
(e.g. computer simulation) and the experimental observations are
Gaussian:
\begin{equation}\label{eq:model}
d_i = \mathcal{M}(\theta) + \epsilon_i,
\end{equation}
where $\theta$ are the parameters of model $\mathcal{M}$ and
$\epsilon_i$ is a random variable that can encompass both measurement
errors on $d_i$ and modeling errors associated with the simulation
$\mathcal{M}(\theta)$. We further assume that all experiments and
observations are independent.  If we have $n$ observations, the
probabilistic model defined by Eq.~(\ref{eq:model}) results in a
likelihood function for $\theta$ that is the product of $n$ normal
probability density functions as shown in
Equation~\ref{eqn:Likelihood}.
\begin{equation}\label{eqn:Likelihood}  
\mathcal{L}({\theta};d) = \prod_{i=1}^n \frac{1}{\sigma \sqrt{2\pi}} \exp
\left[ - \frac{\left(d_i-\mathcal{M}({\theta})\right)^2}{2\sigma^2} \right]
\end{equation}

The negative log-likelihood is known as the misfit function:
\begin{equation}
  {\text M}\left( {\theta ;d} \right) = \frac{1}{2} r^T \Gamma_d^{-1} r
\end{equation}
where the residual $r$ is defined from the difference between model 
and data
\begin{equation}
r = \mathcal{M}(\theta) - d
\end{equation}
and $\Gamma_d$ is the covariance matrix of the Gaussian data
uncertainties.  Dropping $\Gamma_d$ from this expression (or
equivalently, taking it to be the identity) results in ordinary least
squares (OLS).  Minimizing this misfit function results in a point 
known as the maximum likelihood estimate (MLE), which will be the
same as the OLS estimate for conditions of constant data covariance.

When incorporating the prior density, the maximum a posteriori
probability (MAP) point is the point that maximizes the posterior
probability in Equation~\ref{eqn:BayesThm}.  This point will differ
from the MLE for cases of non-uniform prior probability.

\section{Proposal Densities} \label{uq:bayes:prop}

When derivatives are readily available (e.g., from emulator models such
as polynomial chaos, stochastic collocation, or Gaussian processes),
we can form the Hessian of the misfit function using either a full
Hessian 
\begin{equation}
\nabla^2 {\text M} = 
\end{equation}
or a Gauss-Newton approximation\begin{equation}
\nabla^2 {\text M} = 
\end{equation}
