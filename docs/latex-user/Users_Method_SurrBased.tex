\chapter{Surrogate-Based Minimization}\label{sbm}

\section{Overview}\label{sbm:overview}

Surrogate models approximate an original, high fidelity ``truth''
model, typically at reduced computational cost.  In DAKOTA, several
surrogate model selections are possible, which are categorized as data
fits, multifidelity models, and reduced-order models, as described in
Section~\ref{models:surrogate}.  In the context of minimization
(optimization or calibration), surrogate models can speed convergence
by reducing function evaluation cost or smoothing noisy response
functions.

\section{Surrogate-Based Local Minimization}\label{sbm:sblm}

In the surrogate-based local minimization method (keyword:
\texttt{surrogate\_based\_local}) the minimization algorithm operates on
a surrogate model instead of directly operating on the computationally
expensive simulation model. The surrogate model can be based on data
fits, multifidelity models, or reduced-order models, as described in
Section~\ref{models:surrogate}. Since the surrogate will generally
have a limited range of accuracy, the surrogate-based local algorithm
periodically checks the accuracy of the surrogate model against the
original simulation model and adaptively manages the extent of the
approximate optimization cycles using a trust region approach.

%The surrogate-based local method in
%DAKOTA can be implemented using heuristic rules (less expensive) or
%provably-convergent rules (more expensive). The heuristic approach
%is particularly effective on real-world engineering design problems
%that contain nonsmooth features (e.g., slope discontinuities,
%numerical noise) where gradient-based optimization methods often have
%trouble, and where the computational expense of the simulation
%precludes the use of nongradient-based methods.

A generally-constrained nonlinear programming problem takes the form
\begin{eqnarray}
{\rm minimize } \hfil & f({\bf x}) \nonumber \\
{\rm subject\  to } & {\bf g}_l \le {\bf g}({\bf x}) \le {\bf g}_u \nonumber \\
		    &               {\bf h}({\bf x}) = {\bf h}_t \nonumber \\
		    & {\bf x}_l \le {\bf x} \le {\bf x}_u
\label{eq:NLP_standard}
\end{eqnarray}
where ${\bf x} \in \Re^n$ is the vector of design variables, and $f$,
${\bf g}$, and ${\bf h}$ are the objective function, nonlinear
inequality constraints, and nonlinear equality constraints,
respectively\footnote{Any linear constraints are not approximated and
may be added without modification to all formulations}.  Individual
nonlinear inequality and equality constraints are enumerated using $i$
and $j$, respectively (e.g., $g_i$ and $h_j$).  The corresponding
surrogate-based optimization (SBO) algorithm may be formulated in
several ways and applied to either optimization or least-squares
calibration problems. In all cases, SBO solves a sequence of $k$
approximate optimization subproblems subject to a trust region
constraint $\Delta^k$; however, many different forms of the surrogate
objectives and constraints in the approximate subproblem can be
explored.  In particular, the subproblem objective may be a surrogate
of the original objective or a surrogate of a merit function (most
commonly, the Lagrangian or augmented Lagrangian), and the subproblem
constraints may be surrogates of the original constraints, linearized
approximations of the surrogate constraints, or may be omitted
entirely.  Each of these combinations is shown in
Table~\ref{tab:sbo_subprob}, where black indicates an inappropriate
combination, gray indicates an acceptable combination, and blue
indicates a common combination.
\begin{table}
\centering
\caption{SBO approximate subproblem formulations.} \label{tab:sbo_subprob}
\begin{tabular}{c|c|c|c|}
%             & Original  &            & Augmented  \\
       & Original Objective & Lagrangian & Augmented Lagrangian \\
\hline
No constraints         & \cellcolor{black}\phantom{Original Objective}
                       & \cellcolor[gray]{0.5} \phantom{Original Objective}
                       & \cellcolor{blue}\textcolor{white}{TRAL} \\
\hline
Linearized constraints & \cellcolor[gray]{0.5}
                       & \cellcolor{blue}\textcolor{white}{SQP-like}
                       & \cellcolor[gray]{0.5} \\
\hline
Original constraints   & \cellcolor{blue}\textcolor{white}{Direct surrogate}
                       & \cellcolor[gray]{0.5}
                       & \cellcolor{blue}\textcolor{white}{IPTRSAO} \\
\hline
\end{tabular}
\end{table}

Initial approaches to nonlinearly-constrained SBO optimized an
approximate merit function which incorporated the nonlinear
constraints~\cite{Rod98,Ale00}:
\begin{eqnarray}
{\rm minimize } & {\hat \Phi}^k({\bf x}) \nonumber \\
{\rm subject\  to } 
	& {\parallel {\bf x} - {\bf x}^k_c \parallel}_\infty \le \Delta^k
\label{eq:NLP_SBO_TRAL}
\end{eqnarray}
where the surrogate merit function is denoted as $\hat \Phi({\bf x})$,
${\bf x}_c$ is the center point of the trust region,
%the initial value for $\Delta$ at $k=0$ is user-selected, 
and the trust region is truncated at the global variable bounds as
needed.  The merit function to approximate was typically chosen to be
a standard implementation~\cite{Van84,Noc99,Gil81} of the
augmented Lagrangian merit function (see
Eqs.~\ref{eq:aug_lag_merit}--\ref{eq:aug_lag_psi}), where the
surrogate augmented Lagrangian is constructed from individual
surrogate models of the objective and constraints (approximate and
assemble, rather than assemble and approximate).  In
Table~\ref{tab:sbo_subprob}, this corresponds to row 1, column 3, and
is known as the trust-region augmented Lagrangian (TRAL) approach.
While this approach was provably convergent, convergence rates to
constrained minima have been observed to be slowed by the required
updating of Lagrange multipliers and penalty
parameters~\cite{Per04a}.  Prior to converging these parameters, SBO
iterates did not strictly respect constraint boundaries and were often
infeasible.  A subsequent approach (IPTRSAO~\cite{Per04a}) that
sought to directly address this shortcoming added explicit surrogate
constraints (row 3, column 3 in Table~\ref{tab:sbo_subprob}):
\begin{eqnarray}
{\rm minimize } & {\hat \Phi}^k({\bf x}) \nonumber \\
{\rm subject\  to } 
	& {\bf g}_l \le {\bf {\hat g}}^k({\bf x}) \le {\bf g}_u \nonumber \\
	&               {\bf {\hat h}}^k({\bf x}) = {\bf h}_t \nonumber \\
	& {\parallel {\bf x} - {\bf x}^k_c \parallel}_\infty \le \Delta^k \; . 
\label{eq:NLP_SBO_TRAL2}
\end{eqnarray}
While this approach does address infeasible iterates, it still shares
the feature that the surrogate merit function may reflect inaccurate
relative weightings of the objective and constraints prior to
convergence of the Lagrange multipliers and penalty parameters.  That
is, one may benefit from more feasible intermediate iterates, but the
process may still be slow to converge to optimality.  The concept of
this approach is similar to that of SQP-like SBO
approaches~\cite{Ale00} which use linearized constraints:
\begin{eqnarray}
{\rm minimize } & {\hat \Phi}^k({\bf x}) \nonumber \\
{\rm subject\  to } 
& {\bf g}_l \le {\bf {\hat g}}^k({\bf x}^k_c) + 
\nabla {\bf {\hat g}}^k({\bf x}^k_c)^T ({\bf x} - {\bf x}^k_c) \le {\bf g}_u 
\nonumber \\
& {\bf {\hat h}}^k({\bf x}^k_c) + \nabla {\bf {\hat h}}^k({\bf x}^k_c)^T 
({\bf x} - {\bf x}^k_c) = {\bf h}_t \nonumber \\
& {\parallel {\bf x} - {\bf x}^k_c \parallel}_\infty \le \Delta^k \; . 
\label{eq:NLP_SBO_SQP}
\end{eqnarray}
in that the primary concern is minimizing a composite merit function
of the objective and constraints, but under the restriction that the
original problem constraints may not be wildly violated prior to
convergence of Lagrange multiplier estimates.  Here, the merit
function selection of the Lagrangian function (row 2, column 2 in
Table~\ref{tab:sbo_subprob}; see also Eq.~\ref{eq:lag_merit}) is most
closely related to SQP, which includes the use of first-order Lagrange
multiplier updates (Eq.~\ref{eq:lls_lambda}) that should converge more
rapidly near a constrained minimizer than the zeroth-order updates
(Eqs.~\ref{eq:lambda_psi}-\ref{eq:lambda_h}) used for the augmented
Lagrangian.

All of these previous constrained SBO approaches involve a recasting
of the approximate subproblem objective and constraints as a function
of the original objective and constraint surrogates.  A more direct
approach is to use a formulation of:
\begin{eqnarray}
{\rm minimize } & {\hat f}^k({\bf x}) \nonumber \\
{\rm subject\  to } 
	& {\bf g}_l \le {\bf {\hat g}}^k({\bf x}) \le {\bf g}_u \nonumber \\
	&               {\bf {\hat h}}^k({\bf x}) = {\bf h}_t \nonumber \\
	& {\parallel {\bf x} - {\bf x}^k_c \parallel}_\infty \le \Delta^k 
\label{eq:NLP_SBO_direct}
\end{eqnarray}
This approach has been termed the direct surrogate approach since it
optimizes surrogates of the original objective and constraints (row 3,
column 1 in Table~\ref{tab:sbo_subprob}) without any recasting.  It is
attractive both from its simplicity and potential for improved performance,
%assuming that all of the trust region updating machinery can be rendered
%compatible with the lack of an explicitly-optimized merit function.
and is the default approach taken in DAKOTA.  Other DAKOTA defaults
include the use of a filter method for iterate acceptance (see
Section~\ref{sbm:sblm_con_iter}), an augmented Lagrangian merit
function (see Section~\ref{sbm:sblm_con_merit}), Lagrangian hard
convergence assessment (see Section~\ref{sbm:sblm_con_hard}), and no
constraint relaxation (see Section~\ref{sbm:sblm_con_relax}).

While the formulation of Eq.~\ref{eq:NLP_SBO_TRAL} (and others from
row 1 in Table~\ref{tab:sbo_subprob}) can suffer from infeasible
intermediate iterates and slow convergence to constrained minima, each
of the approximate subproblem formulations with explicit constraints
(Eqs.~\ref{eq:NLP_SBO_TRAL2}-\ref{eq:NLP_SBO_direct}, and others from
rows 2-3 in Table~\ref{tab:sbo_subprob}) can suffer from the lack of a
feasible solution within the current trust region.  Techniques for
dealing with this latter challenge involve some form of constraint
relaxation.  Homotopy approaches~\cite{Per04a,Per04b} or composite
step approaches such as Byrd-Omojokun~\cite{Omo89},
Celis-Dennis-Tapia~\cite{Cel85}, or MAESTRO~\cite{Ale00} may be used
for this purpose (see Section~\ref{sbm:sblm_con_relax}).

After each of the $k$ iterations in the SBO method, the predicted
step is validated by computing $f({\bf x}^k_\ast)$,
${\bf g}({\bf x}^k_\ast)$, and ${\bf h}({\bf x}^k_\ast)$.  One
approach forms the trust region ratio $\rho^k$ which measures the
ratio of the actual improvement to the improvement predicted by
optimization on the surrogate model.  When optimizing on an approximate 
merit function (Eqs.~\ref{eq:NLP_SBO_TRAL}--\ref{eq:NLP_SBO_SQP}), the 
following ratio is natural to compute
\begin{equation}
\rho^k = \frac{     \Phi({\bf x}^k_c)      - \Phi({\bf x}^k_\ast)}
	      {\hat \Phi({\bf x}^k_c) - \hat \Phi({\bf x}^k_\ast)} \; .
\label{eq:rho_phi_k}
\end{equation}
The formulation in Eq.~\ref{eq:NLP_SBO_direct} may also form a merit
function for computing the trust region ratio; however, the omission
of this merit function from explicit use in the approximate
optimization cycles can lead to synchronization problems with the
optimizer.  %In this case, penalty-free and multiplier-free trust
%region ratios (see Section~\ref{sbm:sblm_con_merit}) become attractive.

Once computed, the value for $\rho^k$ can be used to define the step
acceptance and the next trust region size $\Delta^{k+1}$ using logic
similar to that shown in Table~\ref{tab:rho_k}.  Typical factors for
shrinking and expanding are 0.5 and 2.0, respectively, but these as
well as the threshold ratio values are tunable parameters in the
algorithm (see Surrogate-Based Method controls in the DAKOTA Reference
Manual~\cite{RefMan}).  In addition, the use of discrete thresholds is
not required, and continuous relationships using adaptive logic can
also be explored~\cite{Wuj98a,Wuj98b}.  Iterate acceptance or
rejection completes an SBO cycle, and the cycles are continued until
either soft or hard convergence criteria (see
Section~\ref{sbm:sblm_con_hard}) are satisfied.

\begin{table}
\centering
\caption{Sample trust region ratio logic.}
\label{tab:rho_k}
\begin{tabular}{cccc}
\hline
Ratio Value & Surrogate Accuracy & Iterate Acceptance & Trust Region Sizing \\
\hline
$\rho^k \le 0$ 
& poor
& reject step 
& shrink \\
$0 < \rho^k \le 0.25$
& marginal
& accept step
& shrink \\
$0.25 < \rho^k < 0.75$ or $\rho^k > 1.25$
& moderate
& accept step
& retain \\
$0.75 \le \rho^k \le 1.25$
& good
& accept step
& expand\footnotemark \\
\hline
\end{tabular}
\end{table}\footnotetext[2]{Exception: retain if ${\bf x}^k_\ast$ in 
trust region interior for design of experiments-based surrogates 
(global data fits, S-ROM, global E-ROM)}


%\subsection{Constraint Management in SBO} \label{sbm:sblm_con}

\subsection{Iterate acceptance logic} \label{sbm:sblm_con_iter}

\begin{wrapfigure}{r}{.3\textwidth}
  \centering
  \includegraphics[width=.3\textwidth]{images/filter}
  \caption{Depiction of filter method.}
  \label{fig:filter}
\end{wrapfigure}
When a surrogate optimization is completed and the approximate
solution has been validated, then the decision must be made to either
accept or reject the step.  The traditional approach is to base this
decision on the value of the trust region ratio, as outlined
previously in Table~\ref{tab:rho_k}.  An alternate approach is to
utilize a filter method~\cite{Fle02}, which does not require
penalty parameters or Lagrange multiplier estimates.  The basic idea
in a filter method is to apply the concept of Pareto optimality to the
objective function and constraint violations and only accept an
iterate if it is not dominated by any previous
iterate. Mathematically, a new iterate is not dominated if at least
one of the following:
\begin{equation}
{\rm either~~~} f < f^{(i)} {\rm ~~~or~~~} c < c^{(i)}
%  if (new_f >= filt_f && new_g >= filt_g)
%    return false;            // new point is dominated: reject iterate
%  else if (new_f < filt_f && new_g < filt_g)
%    rm_list.insert(filt_it); // old pt dominated by new: queue for removal
\end{equation}
is true for all $i$ in the filter, where $c$ is a selected norm of the
constraint violation.  This basic description can be augmented with
mild requirements to prevent point accumulation and assure
convergence, known as a slanting filter~\cite{Fle02}.
Figure~\ref{fig:filter} illustrates the filter concept, where
objective values are plotted against constraint violation for accepted
iterates (blue circles) to define the dominated region (denoted by the
gray lines). A filter method relaxes the common enforcement of
monotonicity in constraint violation reduction and, by allowing more
flexibility in acceptable step generation, often allows the algorithm
to be more efficient.
% Note: filter method idea could allow even more flexibility with
% elimination of the reduction of individual constraint violations
% into a single norm.  That is, the Pareto concept could be
% extended to N_con + 1 dimensions.  However, without another
% mechanism to enforce violation reduction, the algorithm could
% easily generate steps that are acceptable to the filter but which
% diverge in constraint violation.

The use of a filter method is compatible with any of the SBO
formulations in Eqs.~\ref{eq:NLP_SBO_TRAL}--\ref{eq:NLP_SBO_direct}.
%; however, it is particularly attractive for the latter since the only
%remaining purpose for a merit function is for managing trust region
%expansion/retention/contraction when the filter accepts a step.
%If alternate logic can be developed
%for that portion, then the entire SBO algorithm can become penalty and
%multiplier free.  In~\cite{Fle02}, for example, trust
%region updates are less structured than in Table~\ref{tab:rho_k} and
%only basic logic is provided (no $\rho^k$ is used).

\subsection{Merit functions} \label{sbm:sblm_con_merit}

%Merit functions are used in the trust region ratio calculations 
%for sizing subsequent trust regions.  They may also be used for the
%surrogate objective function as described in~\cite{Rod98,Ale00,Per04b},
%which has the advantage of better synchronizing the trust region ratios
%with the approximate optimization steps, but which has the disadvantage
%that it can slow convergence.

The merit function $\Phi({\bf x})$ used in
Eqs.~\ref{eq:NLP_SBO_TRAL}-\ref{eq:NLP_SBO_SQP},\ref{eq:rho_phi_k} may be
selected to be a penalty function, an adaptive penalty function, a
Lagrangian function, or an augmented Lagrangian function.  In each of
these cases, the more flexible inequality and equality constraint
formulations with two-sided bounds and targets
(Eqs.~\ref{eq:NLP_standard},\ref{eq:NLP_SBO_TRAL2}-\ref{eq:NLP_SBO_direct}), 
have been converted to a standard form of ${\bf g}({\bf x}) \le 0$ and
${\bf h}({\bf x}) = 0$ (in
Eqs.~\ref{eq:penalty_merit},\ref{eq:lag_merit}-\ref{eq:lls_lambda}).
The active set of inequality constraints is denoted as ${\bf g}^+$.

The penalty function employed in this paper uses a quadratic penalty
with the penalty schedule linked to SBO iteration number
\begin{eqnarray}
\Phi({\bf x}, r_p) & = & f({\bf x})
%+ \sum_{i=1}^{n_g} r_p (g_i^+({\bf x}))^2
%+ \sum_{i=1}^{n_h} r_p (h_i^+({\bf x}))^2
+ r_p {\bf g}^+({\bf x})^T {\bf g}^+({\bf x})
+ r_p {\bf h}({\bf x})^T {\bf h}({\bf x}) \label{eq:penalty_merit} \\
r_p & = & e^{(k + {\rm offset})/10} % static offset = 21 gives r_p ~ 8 for k = 0
\label{eq:exp_rp}
\end{eqnarray}
The adaptive penalty function is identical in form to
Eq.~\ref{eq:penalty_merit}, but adapts $r_p$ using monotonic increases
in the iteration offset value in order to accept any iterate that
reduces the constraint violation.

The Lagrangian merit function is
\begin{equation}
\Phi({\bf x}, \mbox{\boldmath $\lambda$}_g, \mbox{\boldmath
$\lambda$}_h) = f({\bf x})
%+ \sum_{i=1}^{n_g} (\lambda_i g_i({\bf x})
%+ \sum_{i=1}^{n_h} (\lambda_i h_i({\bf x})
+ \mbox{\boldmath $\lambda$}_g^T {\bf g}^+({\bf x})
+ \mbox{\boldmath $\lambda$}_h^T {\bf h}({\bf x}) \label{eq:lag_merit}
\end{equation}
for which the Lagrange multiplier estimation is discussed in
Section~\ref{sbm:sblm_con_hard}.
% Defer this?:
Away from the optimum, it is possible for the least squares estimates
of the Lagrange multipliers for active constraints to be zero, which
equates to omitting the contribution of an active constraint from the
merit function.  This is undesirable for tracking SBO progress, so
usage of the Lagrangian merit function is normally restricted to
approximate subproblems and hard convergence assessments.

The augmented Lagrangian employed in this paper follows the sign
conventions described in~\cite{Van84}
\begin{eqnarray}
\Phi({\bf x}, \mbox{\boldmath $\lambda$}_{\psi}, \mbox{\boldmath
$\lambda$}_h, r_p) & = & f({\bf x})
%+ \sum_{i=1}^{n_g} (\lambda_i g_i({\bf x}) + r_p (g_i^+({\bf x}))^2)
%+ \sum_{i=1}^{n_h} (\lambda_i h_i({\bf x}) + r_p (h_i^+({\bf x}))^2)
+ \mbox{\boldmath $\lambda$}_{\psi}^T \mbox{\boldmath $\psi$}({\bf x})
+ r_p \mbox{\boldmath $\psi$}({\bf x})^T \mbox{\boldmath $\psi$}({\bf x})
+ \mbox{\boldmath $\lambda$}_h^T {\bf h}({\bf x})
+ r_p {\bf h}({\bf x})^T {\bf h}({\bf x}) \label{eq:aug_lag_merit} \\
\psi_i & = & \max\left\{g_i, -\frac{\lambda_{\psi_i}}{2r_p}\right\}
\label{eq:aug_lag_psi}
\end{eqnarray}
where {\boldmath $\psi$}({\bf x}) is derived from the elimination of
slack variables for the inequality constraints.  In this case, simple
zeroth-order Lagrange multiplier updates may be used:
\begin{eqnarray}
\mbox{\boldmath $\lambda$}_{\psi}^{k+1} & = & \mbox{\boldmath
$\lambda$}_{\psi}^k + 2r_p\mbox{\boldmath $\psi$}({\bf x})
\label{eq:lambda_psi} \\ 
\mbox{\boldmath $\lambda$}_h^{k+1} & = & \mbox{\boldmath $\lambda$}_h^k 
+ 2 r_p {\bf h}({\bf x})
\label{eq:lambda_h}
\end{eqnarray}
The updating of multipliers and penalties is carefully
orchestrated~\cite{Con00} to drive reduction in constraint
violation of the iterates.  The penalty updates can be more
conservative than in Eq.~\ref{eq:exp_rp}, often using an infrequent
application of a constant multiplier rather than a fixed exponential
progression.

%As mentioned previously, a goal for the formulation in
%Eq.~\ref{eq:NLP_SBO_direct} is to employ a penalty and multiplier free
%approach for the merit function and/or trust region logic.  A
%Lagrangian merit function is penalty free and a penalty merit function
%is multiplier free, but no merit functions to this point are both.
%One concept~\cite{Giu00} is to bypass the need for a merit function by
%forming a set of trust region ratios, one for each surrogate function
%(${\hat f}$, ${\hat g}_i$, and ${\hat h}_j$).  In this case, a single
%ratio could be determined from the minimum (or average, norm, etc.) of
%the set,
% -----
%The weakness of this approach is one of scaling near optimality/balancing 
%optimality and feasibility: when constraint values are near zero, the 
%feasibility trust region ratios are less important than the optimality 
%trust region ratios.  This is naturally captured in merit function approaches.
% -----
%or a composite step approach could be used with different trust region
%sizes for the constraint reduction and objective reduction
%subproblems~\cite{Ale00}.  Another concept is to utilize a
%merit function derived from the filter concept using, for example, metrics
%of filter area swept out by accepted iterates.  This concept will be 
%investigated further in future work.
%Initial concepts for swept filter area have issues with potential 
%unboundedness, but will be investigated further in future work.

\subsection{Convergence assessment} \label{sbm:sblm_con_hard}

To terminate the SBO process, hard and soft convergence metrics are
monitored.  It is preferable for SBO studies to satisfy hard
convergence metrics, but this is not always practical (e.g., when
gradients are unavailable or unreliable).  Therefore, simple soft
convergence criteria are also employed which monitor for diminishing
returns (relative improvement in the merit function less than a
tolerance for some number of consecutive iterations).
% Note: soft convergence is not discussed in \cite{Giu00} (and can't be cited)

To assess hard convergence, one calculates the norm of the projected
gradient of a merit function whenever the feasibility tolerance is
satisfied.  The best merit function for this purpose is the Lagrangian
merit function from Eq.~\ref{eq:lag_merit}.  This requires a least
squares estimation for the Lagrange multipliers that best minimize the
projected gradient:
\begin{equation}
\nabla_x \Phi({\bf x}, \mbox{\boldmath $\lambda$}_g, \mbox{\boldmath
$\lambda$}_h) = \nabla_x f({\bf x})
%+ \sum_{i=1}^{n_g} (\lambda_i g_i({\bf x})
%+ \sum_{i=1}^{n_h} (\lambda_i h_i({\bf x})
+ \mbox{\boldmath $\lambda$}_g^T \nabla_x {\bf g}^+({\bf x}) +
\mbox{\boldmath $\lambda$}_h^T \nabla_x {\bf h}({\bf x})
\label{eq:lag_merit_grad}
\end{equation}
where gradient portions directed into active global variable bounds
have been removed.  This can be posed as a linear least squares
problem for the multipliers:
\begin{equation}
{\bf A} \mbox{\boldmath $\lambda$} = -\nabla_x f \label{eq:lls_lambda}
\end{equation}
where ${\bf A}$ is the matrix of active constraint gradients,
$\mbox{\boldmath $\lambda$}_g$ is constrained to be non-negative, and
$\mbox{\boldmath $\lambda$}_h$ is unrestricted in sign.  To estimate
the multipliers using non-negative and bound-constrained linear least
squares, the NNLS and BVLS routines~\cite{Law74} from NETLIB are used,
respectively.

\subsection{Constraint relaxation} \label{sbm:sblm_con_relax}

% trConstraintRelax may be COMPOSITE\_STEP or HOMOTOPY.  

The goal of constraint relaxation is to achieve efficiency through the
balance of feasibility and optimality when the trust region
restrictions prevent the location of feasible solutions to constrained
approximate subproblems
(Eqs.~\ref{eq:NLP_SBO_TRAL2}-\ref{eq:NLP_SBO_direct}, and other
formulations from rows 2-3 in Table~\ref{tab:sbo_subprob}).  The SBO
algorithm starting from infeasible points will commonly generate
iterates which seek to satisfy feasibility conditions without regard
to objective reduction~\cite{Per04b}.

One approach for achieving this balance is to use {\em relaxed
constraints} when iterates are infeasible with respect to the
surrogate constraints.  We follow Perez, Renaud, and
Watson~\cite{Per04a}, and use a {\em global homotopy} mapping the
relaxed constraints and the surrogate constraints.  For formulations
in Eqs.~\ref{eq:NLP_SBO_TRAL2} and~\ref{eq:NLP_SBO_direct} (and others
from row 3 in Table~\ref{tab:sbo_subprob}), the relaxed constraints
are defined from
\begin{eqnarray}
{\bf {\tilde g}}^k({\bf x}, \tau) &=& {\bf {\hat g}}^k({\bf x}) + 
(1-\tau){\bf b}_{g} \label{eq:relaxed_ineq}\\
{\bf {\tilde h}}^k({\bf x}, \tau) &=& {\bf {\hat h}}^k({\bf x}) + 
(1-\tau){\bf b}_{h} \label{eq:relaxed_eq}
\end{eqnarray}
For Eq.~\ref{eq:NLP_SBO_SQP} (and others from row 2 in
Table~\ref{tab:sbo_subprob}), the original surrogate constraints 
${\bf {\hat g}}^k({\bf x})$ and ${\bf {\hat h}}^k({\bf x})$ in
Eqs.~\ref{eq:relaxed_ineq}-\ref{eq:relaxed_eq} are replaced with 
their linearized forms (${\bf {\hat g}}^k({\bf x}^k_c) + 
\nabla {\bf {\hat g}}^k({\bf x}^k_c)^T ({\bf x} - {\bf x}^k_c)$ 
and ${\bf {\hat h}}^k({\bf x}^k_c) + \nabla {\bf {\hat h}}^k({\bf x}^k_c)^T 
({\bf x} - {\bf x}^k_c)$, respectively).  The approximate subproblem
is then reposed using the relaxed constraints as
\begin{eqnarray}
{\rm minimize } & {\hat f^k}({\bf x})~~{\rm or}~~{\hat \Phi}^k({\bf x})
\nonumber \\
{\rm subject\  to } 
  & {\bf g}_l \le {\bf {\tilde g}}^k({\bf x},\tau^k) \le {\bf g}_u \nonumber \\
  &               {\bf {\tilde h}}^k({\bf x},\tau^k) = {\bf h}_t \nonumber \\
  & {\parallel {\bf x} - {\bf x}^k_c \parallel}_\infty \le \Delta^k
% & {\bf x}_l \le {\bf x} \le {\bf x}_u \nonumber\\
%  & 0 \le \tau \le 1 
\label{eq:NLP_relaxed}
\end{eqnarray}
in place of the corresponding subproblems in 
Eqs.~\ref{eq:NLP_SBO_TRAL2}-\ref{eq:NLP_SBO_direct}. Alternatively, 
since the relaxation terms are constants for the $k^{th}$ iteration, 
it may be more convenient for the implementation to constrain 
${\bf {\hat g}}^k({\bf x})$ and ${\bf {\hat h}}^k({\bf x})$ (or their
linearized forms) subject to relaxed bounds and targets 
(${\bf {\tilde g}}_l^k$, ${\bf {\tilde g}}_u^k$, ${\bf {\tilde h}}_t^k$).  
The parameter $\tau$ is the homotopy parameter controlling the extent of 
the relaxation: when $\tau=0$, the constraints are fully relaxed, and 
when $\tau=1$, the surrogate constraints are recovered.  The vectors 
${\bf b}_{g}, {\bf b}_{h}$ are chosen so that the starting point, 
${\bf x}^0$, is feasible with respect to the fully relaxed constraints:
% NOTE: these _could_ need updating in the case of global data fits
\begin{eqnarray}
&{\bf g}_l \le {\bf {\tilde g}}^0({\bf x}^0, 0) \le {\bf g}_u \\
&{\bf {\tilde h}}^0({\bf x}^0, 0) =  {\bf h}_t
\end{eqnarray}

At the start of the SBO algorithm, $\tau^0=0$ if ${\bf x}^0$ is
infeasible with respect to the unrelaxed surrogate constraints;
otherwise $\tau^0=1$ (i.e., no constraint relaxation is used).  At the
start of the $k^{th}$ SBO iteration where $\tau^{k-1} < 1$, $\tau^k$
is determined by solving the subproblem
\begin{eqnarray}
{\rm maximize } & \tau^k \nonumber \\
{\rm subject\  to } 
  & {\bf g}_l \le {\bf {\tilde g}}^k({\bf x},\tau^k) \le {\bf g}_u \nonumber \\
  &               {\bf {\tilde h}}^k({\bf x},\tau^k) = {\bf h}_t \nonumber \\
  & {\parallel {\bf x} - {\bf x}^k_c \parallel}_\infty \le \Delta^k \nonumber\\
% & {\bf x}_l \le {\bf x} \le {\bf x}_u \nonumber\\
  & \tau^k \ge 0 \label{eq:tau_max}
\end{eqnarray}
% NOTE 2: 
starting at $({\bf x}^{k-1}_*, \tau^{k-1})$, and then adjusted as follows:
\begin{equation}
\tau^k = \min\left\{1,\tau^{k-1} + \alpha
\left(\tau^{k}_{\max}-\tau^{k-1}\right)\right\}
\end{equation}
The adjustment parameter $0 < \alpha < 1$ is chosen so that that the
feasible region with respect to the relaxed constraints has positive
volume within the trust region.  Determining the optimal value for
$\alpha$ remains an open question and will be explored in future work.

After $\tau^k$ is determined using this procedure, the problem in
Eq.~\ref{eq:NLP_relaxed} is solved for ${\bf x}^k_\ast$.
% Note: could just use $\tau^k$ in previous equations above
If the step is accepted, then the value of $\tau^k$ is 
updated using the current iterate ${\bf x}^k_\ast$ and the validated
constraints ${\bf g}({\bf x}^k_\ast)$ and ${\bf h}({\bf x}^k_\ast)$:
\begin{eqnarray}
\tau^{k} & = \min\left\{1,\min_i \tau_i , \min_j \tau_j \right\} \\
\rm{where}~~
\tau_i & = 1 + \frac{\min \left\{g_i({\bf x}^k_\ast) - g_{l_{i}}, 
g_{u_{i}} - g_i({\bf x}^k_\ast)\right\}}{b_{g_{i}}} \\ 
\tau_j & = 1 - \frac{| h_j({\bf x}^k_\ast) - h_{t_{j}} |}{b_{h_{j}}}
\end{eqnarray}
%\begin{align}
%\tau^{k} & = \min\left\{1,\min_i \tau_i , \min_j \tau_j \right\} \; ,\\
%\intertext{where}
%\tau_i & = \frac{\min \left\{\hat g_i({\bf x}^k) - ({\bf g}_l)_i, 
%({\bf g}_u)_i - \hat g_i({\bf x}^k)\right\}}{b_i^{g}} + 1\\ 
%\tau_j & = \frac{- | \hat h_j({\bf x}^k) - ({\bf h}_t)_j |}{b_j^{h}} + 1 \; .
%\end{align}

\begin{wrapfigure}{r}{.35\textwidth}
  \centering
  \includegraphics[width=.35\textwidth]{images/tau_updates}
  \caption{Illustration of SBO iterates using surrogate (red) and
  relaxed (blue) constraints.}
  \label{fig:constr_relax}
\end{wrapfigure}
Figure~\ref{fig:constr_relax} illustrates the SBO algorithm on a
two-dimensional problem with one inequality constraint starting from
an infeasible point, ${\bf x}^0$.  The minimizer of the problem is
denoted as ${\bf x}^*$.  Iterates generated using the surrogate
constraints are shown in red, where feasibility is achieved first, and
then progress is made toward the optimal point.  The iterates
generated using the relaxed constraints are shown in blue, where a
balance of satisfying feasibility and optimality has been achieved,
leading to fewer overall SBO iterations.
%\begin{figure}[ht!]
%\epsfxsize 3in \centerline{\epsfbox{tau_updates.eps}}
%\caption{Example SBO iterates using surrogate (red) and relaxed (blue)
%constraints.}
%\label{fig:constr_relax}
%\end{figure}

The behavior illustrated in Fig.~\ref{fig:constr_relax} is an example
where using the relaxed constraints over the surrogate constraints may
improve the overall performance of the SBO algorithm by reducing the
number of iterations performed.  This improvement comes at the cost of
solving the minimization subproblem in Eq.~\ref{eq:tau_max}, which can
be significant in some cases (i.e., when the cost of evaluating 
${\bf {\hat g}}^k({\bf x})$ and ${\bf {\hat h}}^k({\bf x})$ is not
negligible, such as with multifidelity or ROM surrogates).  As shown
in the numerical experiments involving the Barnes problem presented 
in ~\cite{Per04a}, 
the directions toward constraint violation
reduction and objective function reduction may be in opposing
directions.  In such cases, the use of the relaxed constraints may
result in an {\em increase} in the overall number of SBO iterations
since feasibility must ultimately take precedence.

\subsection{SBO with Data Fits}\label{sbm:sblm:surface}

When performing SBO with local, multipoint, and global data fit
surrogates, it is necessary to regenerate or update the data fit for
each new trust region.  In the global data fit case, this can mean
performing a new design of experiments on the original high-fidelity
model for each trust region, which can effectively limit the approach
to use on problems with, at most, tens of variables.
Figure~\ref{fig:sbo_df} displays this case.  However, an important
benefit of the global sampling is that the global data fits can tame
poorly-behaved, nonsmooth, discontinuous response variations within
the original model into smooth, differentiable, easily navigated
surrogates.  This allows SBO with global data fits to extract the
relevant global design trends from noisy simulation data.

\begin{wrapfigure}{r}{.3\textwidth}
  \centering
  \includegraphics[width=.3\textwidth]{images/sbo_df}
  \caption{SBO iteration progression for global data fits.}
  \label{fig:sbo_df}
\end{wrapfigure}
When enforcing local consistency between a global data fit surrogate
and a high-fidelity model at a point, care must be taken to balance
this local consistency requirement with the global accuracy of the
surrogate.  In particular, performing a correction on an existing
global data fit in order to enforce local consistency can skew the
data fit and destroy its global accuracy.  One approach for achieving
this balance is to include the consistency requirement within the data
fit process by constraining the global data fit calculation (e.g.,
using constrained linear least squares).  This allows the data fit to
satisfy the consistency requirement while still addressing global
accuracy with its remaining degrees of freedom.
% Use figure from Theresa's paper?  Use equations from notes?
Embedding the consistency within the data fit also reduces the
sampling requirements.  For example, a quadratic polynomial normally
requires at least $(n+1)(n+2)/2$ samples for $n$ variables to perform
the fit.  However, with embedded first-order consistency constraints,
the minimum number of samples is reduced by $n+1$ to $(n^2+n)/2$.
% With gradient information in each sample, this can be further
% reduced to ceil(n/2) samples.
%This corresponds to defining the terms of a symmetric Hessian matrix
%and points to an alternate approach.  Rather than enforcing
%consistency through constrained least squares, one can embed
%consistency directly by employing a Taylor series centered at the
%point of local consistency enforcement and globally estimating the
%higher order terms.  In the quadratic polynomial example, a
%second-order Taylor series with globally estimated Hessian terms
%requires the same $(n^2+n)/2$ samples and directly satisfies
%first-order consistency.  To further reduce sampling requirements in
%this case, one can choose to perform only partial updates (e.g., the
%diagonal) of the Hessian matrix~\cite{Per02}.

% Additional research area: Exploiting variance estimators to guide
% global search (e.g., kriging)

In the local and multipoint data fit cases, the iteration progression
will appear as in Fig.~\ref{fig:sbo_mh}.  Both cases involve a single
new evaluation of the original high-fidelity model per trust region,
with the distinction that multipoint approximations reuse information
from previous SBO iterates.  Like model hierarchy surrogates, these
techniques scale to larger numbers of design variables.  Unlike model
hierarchy surrogates, they generally do not require surrogate
corrections, since the matching conditions are embedded in the
surrogate form (as discussed for the global Taylor series approach
above).  The primary disadvantage to these surrogates is that the
region of accuracy tends to be smaller than for global data fits and
multifidelity surrogates, requiring more SBO cycles with smaller trust
regions.
%In SBO with surface fit functions, a sequence of optimization
%subproblems are evaluated, each of which is confined to a subset of
%the parameter space known as a ``trust region.'' Inside each trust
%region, DAKOTA's data sampling methods are used to evaluate the
%response quantities at a small number (order $10^{1}$ to $10^{2}$) of
%design points. Next, multidimensional surface fitting is performed to
%create a surrogate function for each of the response quantities.
%Finally, optimization is performed using the surrogate functions in
%lieu of the actual response quantities, and the optimizer's search is
%limited to the region inside the trust region bounds. A validation
%procedure is then applied to compare the predicted improvement in the
%response quantities to the actual improvement in the response
%quantities. Based on the results of this validation, the optimum
%design point is either accepted or rejected and the size of the trust
%region is either expanded, contracted, or left unchanged. The sequence
%of optimization subproblems continues until the SBO strategy
%convergence criteria are satisfied
More information on the design of experiments methods is available in
Chapter~\ref{dace}, and the data fit surrogates are described in
Section~\ref{models:surrogate:datafit}.

Figure~\ref{sbm:sblm_rosen} shows a DAKOTA input file that implements
surrogate-based optimization on Rosenbrock's function. This input file
is named \texttt{dakota\_sbo\_rosen.in} in the \texttt{Dakota/test}
directory.  The first method keyword block contains the SBO 
keyword \texttt{surrogate\_based\_local}, plus the commands for
specifying the trust region size and scaling factors. The optimization
portion of SBO, using the CONMIN Fletcher-Reeves conjugate gradient method,
is specified in the following keyword blocks for
\texttt{method}, \texttt{model}, \texttt{variables}, and
\texttt{responses}.  The model used by the optimization method 
specifies that a global surrogate will be used to map variables into
responses (no \texttt{interface} specification is used by the
surrogate model). The global surrogate is constructed using a DACE
method which is identified with the \texttt{`SAMPLING'} identifier.
This data sampling portion of SBO is specified in the final set of
keyword blocks for \texttt{method}, \texttt{model},
\texttt{interface}, and \texttt{responses} (the earlier 
\texttt{variables} specification is reused). This example problem uses 
the Latin hypercube sampling method in the LHS software to select 10
design points in each trust region. A single surrogate model is
constructed for the objective function using a quadratic polynomial.
The initial trust region is centered at the design point
$(x_1,x_2)=(-1.2,1.0)$, and extends $\pm 0.4$ (10\% of the global
bounds) from this point in the $x_1$ and $x_2$ coordinate directions.
\begin{figure}
  \begin{bigbox}
    \begin{tiny}
      \verbatimtabinput[8]{dakota_sbo_rosen.in}
    \end{tiny}
  \end{bigbox}
  \caption{DAKOTA input file for the surrogate-based local optimization
    example.}
  \label{sbm:sblm_rosen}
\end{figure}

If this input file is executed in DAKOTA, it will converge to the
optimal design point at $(x_{1},x_{2})=(1,1)$ in approximately 800
function evaluations. While this solution is correct, it is obtained
at a much higher cost than a traditional gradient-based optimizer
(e.g., see the results obtained from \texttt{dakota\_rosenbrock.in}).
This demonstrates that the SBO method with global data fits is not
really intended for use with smooth continuous optimization problems;
direct gradient-based optimization can be more efficient for such
applications. Rather, SBO with global data fits is best-suited for the
types of problems that occur in engineering design where the response
quantities may be discontinuous, nonsmooth, or may have multiple local
optima~\cite{Giu02}. In these types of engineering design problems,
traditional gradient-based optimizers often are ineffective, whereas
global data fits can extract the global trends of interest despite the
presence of local nonsmoothness (for an example problem with multiple
local optima, look in \texttt{Dakota/test} for the file
\texttt{dakota\_sbo\_sine\_fcn.in}~\cite{Giu00}).

The surrogate-based local minimizer is only mathematically
guaranteed to find a local minimum. However, in practice, SBO can often find 
the global minimum.  Due to the random sampling method used within the
SBO algorithm, the SBO method will solve a given problem a little differently 
each time it is run (unless the user specifies a particular random
number seed in the dakota input file as is shown in Figure~\ref{sbm:sblm_rosen}). 
Our experience on the quasi-sine function mentioned above is that if 
you run this problem 10 times with the same starting conditions but different 
seeds, then you will find the global minimum in about 70-80\% of the trials.
This is good performance for what is mathematically only a local optimization method.

\subsection{SBO with Multifidelity Models}\label{sbm:sblm:multifidelity}

When performing SBO with model hierarchies, the low-fidelity model is
normally fixed, requiring only a single high-fidelity evaluation to
compute a new correction for each new trust region.
Figure~\ref{fig:sbo_mh} displays this case.  This renders the
multifidelity SBO technique more scalable to larger numbers of design
variables since the number of high-fidelity evaluations per iteration
(assuming no finite differencing for derivatives) is independent of
the scale of the design problem.  However, the ability to smooth
poorly-behaved response variations in the high-fidelity model is lost,
and the technique becomes dependent on having a well-behaved
low-fidelity model\footnote{It is also possible to use a hybrid data
fit/multifidelity approach in which a smooth data fit of a noisy low
fidelity model is used in combination with a high fidelity model}.  In
addition, the parameterizations for the low and high-fidelity models
may differ, requiring the use of a mapping between these
parameterizations.  Space mapping, corrected space mapping, POD
mapping, and hybrid POD space mapping are being explored for this
purpose~\cite{Rob06a,Rob06b}.

\begin{wrapfigure}{r}{.3\textwidth}
  \centering
  \includegraphics[width=.3\textwidth]{images/sbo_mh}
  \caption{SBO iteration progression for model hierarchies.}
  \label{fig:sbo_mh}
\end{wrapfigure}
%\begin{figure}
%\epsfxsize 3in
%\centerline{\epsfbox{sbo_mh.eps}}
%\caption{SBO iteration progression for model hierarchies.}
%\label{fig:sbo_mh}
%\end{figure}

When applying corrections to the low-fidelity model, there is no
concern for balancing global accuracy with the local consistency
requirements.  However, with only a single high-fidelity model evaluation
at the center of each trust region, it is critical to use the best
correction possible on the low-fidelity model in order to achieve
rapid convergence rates to the optimum of the high-fidelity
model~\cite{Eld04}.

%SBO can also be applied with multifidelity, or hierarchical, models,
%i.e., where one has available both a high-fidelity computational model
%and a low-fidelity computational model. This situation can occur when
%the low-fidelity model neglects some physical phenomena (e.g.,
%viscosity, heat transfer, etc.) that are included in the high-fidelity
%model, or when the low-fidelity model has a lower resolution
%computational mesh than the high-fidelity model. In many cases, the
%low-fidelity model can serve as a surrogate for the high-fidelity
%model during the optimization process. Thus, the low-fidelity model
%can be used in SBO in a manner similar to the use of surface fit
%models described in Section~\ref{sbm:sblm:surface}. A key difference
%in SBO with hierarchical surrogates is that a design of experiments
%using the high-fidelity model is not required; rather high-fidelity
%evaluations are only needed at the center of the current trust-region
%and the predicted optimum point in order to correct the low-fidelity
%model and verify improvement, respectively. Another difference is that
%one of the four types of correction described in
%Section~\ref{sbm:sblm:surface} is required for SBO with multifidelity
%models.

A multifidelity test problem named
\texttt{dakota\_sbo\_hierarchical.in} is available in
\texttt{Dakota/test} to demonstrate this SBO approach. This test
problem uses the Rosenbrock function as the high fidelity model and a
function named ``lf\_rosenbrock'' as the low fidelity model. Here,
lf\_rosenbrock is a variant of the Rosenbrock function (see
\texttt{Dakota/test/lf\_rosenbrock.C} for formulation) with the
minimum point at $(x_1,x_2)=(0.80,0.44)$, whereas the minimum of the
original Rosenbrock function is $(x_1,x_2)=(1,1)$. Multifidelity SBO
locates the high-fidelity minimum in 11 high fidelity evaluations for
additive second-order corrections and in 208 high fidelity evaluations
for additive first-order corrections, but fails for zeroth-order
additive corrections by converging to the low-fidelity minimum.

\subsection{SBO with Reduced Order Models}\label{sbm:sblm:rom}

When performing SBO with reduced-order models (ROMs), the ROM is
mathematically generated from the high-fidelity model.  A critical
issue in this ROM generation is the ability to capture the effect of
parametric changes within the ROM.  Two approaches to parametric ROM
are extended ROM (E-ROM) and spanning ROM (S-ROM)
techniques~\cite{Wei06}.  Closely related techniques include tensor
singular value decomposition (SVD) methods~\cite{Lat00}.  In the
single-point and multipoint E-ROM cases, the SBO iteration can appear
as in Fig.~\ref{fig:sbo_mh}, whereas in the S-ROM, global E-ROM, and
tensor SVD cases, the SBO iteration will appear as in
Fig.~\ref{fig:sbo_df}.  In addition to the high-fidelity model
analysis requirements, procedures for updating the system matrices and
basis vectors are also required.

Relative to data fits and multifidelity models, ROMs have some
attractive advantages.  Compared to data fits such as regression-based
polynomial models, they are more physics-based and would be expected
to be more predictive (e.g., in extrapolating away from the immediate
data).  Compared to multifidelity models, ROMS may be more practical
in that they do not require multiple computational models or meshes
which are not always available.  The primary disadvantage is potential
invasiveness to the simulation code for projecting the system using
the reduced basis.


\section{Surrogate-Based Global Minimization}\label{sbm:sbgm}

Surrogate-based global minimization differs from the surrogate-based 
local minimization approach discussed in the previous section in several ways: 
it is not a trust-region approach; initially there is one surrogate 
constructed over a set of sample points and the optimizer operates on that 
surrogate (as opposed to adaptively selecting points and re-building a 
surrogate in each trust region); and there is no guarantee of convergence. 

The \texttt{surrogate\_based\_global} method was developed to address
two needs.  The first is the case where a user wishes to use existing
function evaluations or a fixed sample size (perhaps based on
computational cost and allocation of resources) to build a surrogate
once and optimize on it.  In this case (a single global optimization
on a surrogate model), the set of surrogate building points is
determined in advance as opposed to the trust-region local surrogate
optimization in which the number of ``true'' function evaluations
depends on the location and size of the trust region, the goodness of
the surrogate within the trust-region, and problem characteristics.

In the second \texttt{surrogate\_based\_global} use case, we want to
update the surrogate, but globally.  That is, we add points to the
sample set used to create the surrogate, rebuild the surrogate, and
then perform another global optimization on the new surrogate.  Thus,
surrogate-based global optimization can be used in an iterative
scheme.  In one iteration, minimizers of the surrogate model are
found, and a selected subset of these are passed to the next
iteration.  In the next iteration, these surrogate points are
evaluated with the ``truth'' model, and then added to the set of
points upon which the next surrogate is constructed.  This presents a
more accurate surrogate to the minimizer at each subsequent iteration,
presumably driving to optimality quickly.  Note that a global
surrogate is constructed using the same bounds in each iteration.
This approach has no guarantee of convergence.

The surrogate-based global method was originally designed for MOGA (a
multi-objective genetic algorithm).  Since genetic algorithms often
need thousands or tens of thousands of points to produce optimal or
near-optimal solutions, surrogates can help by reducing the necessary
truth model evaluations.  Instead of creating one set of surrogates
for the individual objectives and running the optimization algorithm
on the surrogate once, the idea is to select points along the
(surrogate) Pareto frontier, which can be used to supplement the
existing points.  In this way, one does not need to use many points
initially to get a very accurate surrogate.  The surrogate becomes
more accurate as the iterations progress.

Most single objective optimization methods will return only a single
optimal point.  In that case, only one point from the surrogate model
will be evaluated with the ``true'' function and added to the pointset
upon which the surrogate is based.  In this case, it will take many
iterations of the surrogate-based global optimization for the approach
to converge, and its utility may not be as great as for the
multi-objective case when multiple optimal solutions are passed from
one iteration to the next to supplement the surrogate.  Note that the
user has the option of appending the optimal points from the surrogate
model to the current set of truth points or using the optimal points
from the surrogate model to replace the optimal set of points from the
previous iteration.  Although appending to the set is the default
behavior, at this time we strongly recommend using the option
\texttt{replace\_points} because it appears to be more accurate and
robust.

When using the surrogate-based global method, we first recommend
running one optimization on a single surrogate model. That is, set
\texttt{max\_iterations} to 1.  This will allow one to get a sense of
where the optima are located and also what surrogate types are the
most accurate to use for the problem.  Note that by fixing the seed of
the sample on which the surrogate is built, one can take a DAKOTA
input file, change the surrogate type, and re-run the problem without
any additional function evaluations by specifying the use of the
dakota restart file which will pick up the existing function
evaluations, create the new surrogate type, and run the optimization
on that new surrogate.  Also note that one can specify that surrogates
be built for all primary functions and constraints or for only a
subset of these functions and constraints.  This allows one to use a
"truth" model directly for some of the response functions, perhaps due
to them being much less expensive than other functions.  Finally, a
diagnostic threshold can be used to stop the method if the surrogate
is so poor that it is unlikely to provide useful points.  If the
goodness-of-fit has an R-squared value less than 0.5, meaning that
less than half the variance of the output can be explained or
accounted for by the surrogate model, the surrogate-based global
optimization stops and outputs an error message.  This is an arbitrary
threshold, but generally one would want to have an R-squared value as
close to 1.0 as possible, and an R-squared value below 0.5 indicates a
very poor fit.

For the surrogate-based global method, we initially recommend a small
number of maximum iterations, such as 3--5, to get a sense of how the
optimization is evolving as the surrogate gets updated globally.  If
it appears to be changing significantly, then a larger number (used in
combination with restart) may be needed.

Figure~\ref{sbm:sbgm_moga} shows a DAKOTA input file that implements
surrogate-based global optimization on a multi-objective test function. 
This input file
is named \texttt{dakota\_su\_mogatest1.in} in the \texttt{Dakota/test}
directory.  The first method keyword block contains the
keyword \texttt{surrogate\_based\_global}, plus the commands for
specifying five as the maximum iterations and the option to replace 
points in the global surrogate construction. The method block identified 
as MOGA specifies a multi-objective genetic algorithm optimizer and its 
controls.  The model keyword block specifies a surrogate model.  
In this case, a \texttt{gaussian\_process} model is used as a surrogate. 
The \texttt{dace\_method\_pointer} specifies that the surrogate will be 
build on 100 Latin Hypercube samples with a seed = 531.
The remainder of the input specification deals with the interface 
to the actual analysis driver and the 2 responses being returned 
as objective functions from that driver. 

\begin{figure}
  \begin{bigbox}
    \begin{tiny}
      \verbatimtabinput[8]{dakota_su_mogatest1.in}
    \end{tiny}
  \end{bigbox}
  \caption{DAKOTA input file for the surrogate-based global optimization
    example.}
  \label{sbm:sbgm_moga}
\end{figure}
 
\section{Efficient Global Minimization}\label{sbm:egm}

Efficient Global Optimization (EGO) is a global optimization technique
that employs response surface surrogates~\cite{Jon98,Hua06}.  In each
EGO iteration, a Gaussian process (GP) approximation for the objective
function is constructed based on sample points of the true simulation.
The GP allows one to specify the prediction at a new input location as
well as the uncertainty associated with that prediction.  The key idea
in EGO is to maximize an Expected Improvement Function (EIF), defined
as the expectation that any point in the search space will provide a
better solution than the current best solution, based on the expected
values and variances predicted by the GP model.  It is important to
understand how the use of this EIF leads to optimal solutions.  The
EIF indicates how much the objective function value at a new potential
location is expected to be less than the predicted value at the
current best solution.  Because the GP model provides a Gaussian
distribution at each predicted point, expectations can be calculated.
Points with good expected values and even a small variance will have a
significant expectation of producing a better solution (exploitation),
but so will points that have relatively poor expected values and
greater variance (exploration).  The EIF incorporates both the idea of
choosing points which minimize the objective and choosing points about
which there is large prediction uncertainty (e.g., there are few or no
samples in that area of the space, and thus the probability may be
high that a sample value is potentially lower than other values).
Because the uncertainty is higher in regions of the design space with
few observations, this provides a balance between exploiting areas of
the design space that predict good solutions, and exploring areas
where more information is needed.

There are two major differences between our implementation and that of
~\cite{Jon98}: we do not use a branch and bound method to find points
which maximize the EIF.  Rather, we use the DIRECT algorithm.  Second,
we allow for multiobjective optimization and nonlinear least squares
including general nonlinear constraints.  Constraints are handled
through an augmented Lagrangian merit function approach (see
Section~\ref{sbm:sblm_con_merit} and
Eqs.~\ref{eq:aug_lag_merit}-\ref{eq:lambda_h}).

The method is specified as \texttt{efficient\_global}.  Currently we
do not expose any specification controls for the underlying Gaussian
process model used or for the optimization of the expected improvement
function, which is currently performed by the NCSU DIRECT
algorithm. The only item the user can specify is a seed which is 
used in the Latin Hypercube Sampling to generate the initial 
set of points which is used to construct the initial Gaussian process. 
An example specification for the EGO algorithm is shown in
Figure~\ref{sbm:egm_rosen}:
\begin{figure}
  \begin{bigbox}
    \begin{small}
      \verbatimtabinput[8]{dakota_rosenbrock_ego.in}
    \end{small}
  \end{bigbox}
  \caption{DAKOTA input file for the efficient global optimization example.}
  \label{sbm:egm_rosen}
\end{figure}

