\chapter{DAKOTA Usage Guidelines}\label{usage}

\section{Problem Exploration}\label{usage:exploration}

The first objective in an analysis is to characterize the problem so
that appropriate algorithms can be chosen. In the case of
optimization, typical questions that should be addressed include: Are
the design variables continuous, discrete, or mixed? Is the problem
constrained or unconstrained? How expensive are the response functions
to evaluate? Will the response functions behave smoothly as the design
variables change or will there be nonsmoothness and/or
discontinuities? Are the response functions likely to be multimodal,
such that global optimization may be warranted? Is analytic gradient
data available, and if not, can I calculate gradients accurately and
cheaply? Additional questions that are pertinent for characterization
of uncertainty quantification problems include: Can I accurately model
the probabilistic distributions of my uncertain variables? Are the
response functions relatively linear? Am I interested in a full random
process characterization of the response functions, or just
statistical results?

If there is not sufficient information from the problem description to
answer these questions, then additional problem characterization
activities may be warranted. One particularly useful characterization
activity that DAKOTA enables is parameter space exploration through
the use of parameter studies and design of experiments methods. The
parameter space can be systematically interrogated to create
sufficient information to evaluate the trends in the response
functions and to determine if these trends are noisy or smooth,
unimodal or multimodal, relatively linear or highly nonlinear, etc. In
addition, the parameter studies may reveal that one or more of the
parameters do not significantly affect the results and can be removed
from the problem formulation. This can yield a potentially large
savings in computational expense for the subsequent studies. Refer to
Chapters~\ref{ps} and~\ref{dace} for additional information on
parameter studies and design of experiments methods.

\section{Optimization Method Selection}\label{usage:selection}

In selecting an optimization method, important considerations include
the type of variables in the problem (continuous, discrete, mixed),
whether a global search is needed or a local search is sufficient, and
the required constraint support (unconstrained, bound constrained,
or generally constrained). Less obvious, but equally important,
considerations include the efficiency of convergence to an optimum
(i.e., convergence rate) and the robustness of the method in the
presence of challenging design space features (e.g., nonsmoothness).

Gradient-based optimization methods are highly efficient, with the
best convergence rates of all of the optimization methods. If analytic
gradient and Hessian information can be provided by an application
code, a full Newton method will provide quadratic convergence rates
near the solution. More commonly, only gradient information is
available and a quasi-Newton method is chosen in which the Hessian
information is approximated from an accumulation of gradient data. In
this case, superlinear convergence rates can be obtained. These
characteristics make gradient-based optimization methods the methods
of choice when the problem is smooth, unimodal, and
well-behaved. However, when the problem exhibits nonsmooth,
discontinuous, or multimodal behavior, these methods can also be the
least robust since inaccurate gradients will lead to bad search
directions, failed line searches, and early termination, and the
presence of multiple minima will be missed.

Thus, for gradient-based optimization, a critical factor is the
gradient accuracy. Analytic gradients are ideal, but are often
unavailable. For many engineering applications, a finite difference
method will be used by the optimization algorithm to estimate gradient
values. DAKOTA allows the user to select the step size for these
calculations, as well as choose between forward-difference and
central-difference algorithms. The finite difference step size should
be selected as small as possible, to allow for local accuracy and
convergence, but not so small that the steps are ``in the noise.'' 
This requires an assessment of the local smoothness of the response
functions using, for example, a parameter study method.  Central
differencing, in general, will produce more reliable gradients than
forward differencing, but at roughly twice the expense.

Nongradient-based methods exhibit much slower convergence rates for
finding an optimum, and as a result, tend to be much more
computationally demanding than gradient-based methods. Nongradient
local optimization methods, such as pattern search algorithms, often
require from several hundred to a thousand or more function
evaluations, depending on the number of variables, and nongradient
global optimization methods such as genetic algorithms may require
from thousands to tens-of-thousands of function evaluations. Clearly,
for nongradient optimization studies, the computational cost of the
function evaluation must be relatively small in order to obtain an
optimal solution in a reasonable amount of time. In addition,
nonlinear constraint support in nongradient methods is an open area of
research and, while supported by many nongradient methods in DAKOTA,
is not as refined as constraint support in gradient-based
methods. However, nongradient methods can be more robust and more
inherently parallel than gradient-based approaches. They can be
applied in situations were gradient calculations are too expensive or
unreliable. In addition, some nongradient-based methods can be used
for global optimization which gradient-based techniques, by
themselves, cannot. For these reasons, nongradient-based methods
deserve consideration when the problem may be nonsmooth, multimodal,
or poorly behaved.

Approaches that seek to improve the effectiveness or efficiency of
optimizers and least squares methods through the use of surrogate
models include the surrogate-based local, surrogate-based global, and
efficient global methods.  Chapter~\ref{sbm} provides further
information on these approaches.  The surrogate-based local approach
(see Section~\ref{sbm:sblm}) brings the efficiency of gradient-based
optimization/least squares methods to nonsmooth or poorly behaved
problems by smoothing noisy or discontinuous response results with a
data fit surrogate model (e.g., a quadratic polynomial) and then
minimizing on the smooth surrogate using efficient gradient-based
techniques.  The surrogate-based global approach (see
Section~\ref{sbm:sbgm}) similarly employs optimizers/least squares
methods with surrogate models, but rather than localizing through the
use of trust regions, seeks global solutions using global methods.
And the efficient global approach (see Section~\ref{sbm:egm}) uses the
specific combination of Gaussian process surrogate models in
combination with the DIRECT global optimizer.  Similar to these
surrogate-based approaches, the hybrid and multistart optimization
strategies seek to bring the efficiency of gradient-based optimization
methods to global optimization problems.  In the former case, a global
optimization method can be used for a few cycles to locate promising
regions and then local gradient-based optimization is used to
efficiently converge on one or more optima. In the latter case, a
stratification technique is used to disperse a series of local
gradient-based optimization runs through parameter space.  Without
surrogate data smoothing, however, these strategies are best for
smooth multimodal problems. Section~\ref{strat:hybrid} and
Section~\ref{strat:multistart} provide more information on these
approaches.

Table~\ref{usage:guideopt} provides a convenient reference for
choosing an optimization method or strategy to match the
characteristics of the user's problem, where blank fields inherit the
value from above. With respect to constraint support, it should be
understood that the methods with more advanced constraint support are
also applicable to the lower constraint support levels; they are
listed only at their highest level of constraint support for brevity.

\begin{table}
\centering
\caption{Guidelines for optimization and nonlinear least squares method 
selection.}
\label{usage:guideopt}\vspace{2mm}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Variable} & \textbf{Function} & \textbf{Solution} &
\textbf{Constraints} & \textbf{Applicable Methods} \\
\textbf{Type} & \textbf{Surface} & \textbf{Type} & & \\

\hline
continuous & smooth & local opt & none & optpp\_cg \\
\hline
      & & & bounds   & dot\_bfgs, dot\_frcg, conmin\_frcg \\
\hline
      & & & general  & npsol\_sqp, nlpql\_sqp, dot\_mmfd, dot\_slp, \\
      & & &          & dot\_sqp, conmin\_mfd, optpp\_newton, \\
      & & &          & optpp\_q\_newton, optpp\_fd\_newton \\
\hline
      & & local least sq & bounds  & nl2sol \\
\hline
      & &                & general & nlssol\_sqp, optpp\_g\_newton \\
\hline
      & & local multiobjective & general & weighted sums (one soln), \\
      & &                      &      & pareto\_set strategy (multiple solns) \\
%\hline
%     & & local large- & general & (planned: reduced\_sqp) \\
%     & & scale opt    &         &                         \\
\hline
      & & global opt      & general & hybrid strategy, multi\_start strategy \\
\hline
      & & global least sq & general & hybrid strategy, multi\_start strategy \\
%\hline
%      & nonsmooth & local opt & none & \\
\hline
      & nonsmooth & local opt & bounds & optpp\_pds \\
\hline
      & & & general & asynch\_pattern\_search, coliny\_cobyla, \\
      & & &         & coliny\_pattern\_search, coliny\_solis\_wets \\
\hline
      & & local/global opt      & general & surrogate\_based\_local \\
\hline
      & & local/global least sq & general & surrogate\_based\_local \\
\hline
      & & global opt            & bounds  & ncsu\_direct \\
\hline
      & &          & general & coliny\_ea, coliny\_direct, efficient\_global, \\
      & &          &         & soga, surrogate\_based\_global \\
\hline
  & & global least sq & general & efficient\_global, surrogate\_based\_global \\
\hline
      & & global multiobjective & general & moga (multiple solns) \\
\hline
discrete       & n/a & global opt & general  & soga, coliny\_ea \\
categorical    &     &            &          & \\
\hline
               &     & global multiobjective & general & moga (multiple solns)\\
\hline
discrete       & n/a & local opt  & general  & branch\_and\_bound strategy \\
noncategorical &     &            &          & \\
\hline
mixed          & nonsmooth & global opt & general & soga, coliny\_ea\\
categorical    &           &            &         & \\
\hline
               & & global multiobjective & general & moga (multiple solns) \\
\hline
mixed          & smooth  & local opt & general & branch\_and\_bound strategy \\
noncategorical &         &           &         & \\
\hline
\end{tabular}
\end{table}

\section{UQ Method Selection}\label{usage:uq}

The need for computationally efficient methods is further amplified in
the case of the quantification of uncertainty in computational
simulations. Sampling-based methods are the most robust uncertainty
techniques available, are applicable to almost all simulations, and
possess rigorous error bounds; consequently, they should be used
whenever the function is relatively inexpensive to compute. However,
in the case of terascale computational simulations, the number of
function evaluations required by traditional techniques such as Monte
Carlo and Latin hypercube sampling (LHS) quickly becomes prohibitive,
especially if tail statistics are needed.  Additional sampling options
include quasi-Monte Carlo (QMC) sampling and importance sampling (IS), 
%or Markov Chain Monte Carlo (MCMC) sampling
and incremental sampling may also be used to incrementally add samples 
to an existing sample set.

Alternatively, one can apply the traditional sampling techniques to a
surrogate function approximating the expensive computational
simulation (see Section~\ref{adv_models:sbuq}). However, if this
approach is selected, the user should be aware that it is very
difficult to assess the accuracy of the results obtained. Unlike
the case of surrogate-based local minimization (see
Section~\ref{sbm:sblm}), there is no simple pointwise calculation to
verify the accuracy of the approximate results. This is due to the
functional nature of uncertainty quantification, i.e. the accuracy of
the surrogate over the entire parameter space needs to be considered,
not just around a candidate optimum as in the case of surrogate-based
local. This issue especially manifests itself when trying to estimate
low probability events such as the catastrophic failure of a system.

Another class of UQ methods known as local reliability methods (e.g.,
MV, AMV/AMV$^2$, AMV+/AMV$^2$+, TANA, and FORM/SORM) are more
computationally efficient in general than the sampling methods and are
effective when applied to reasonably well-behaved response functions;
i.e., functions that are smooth, unimodal, and linear or mildly
nonlinear. They can be used to provide qualitative sensitivity
information concerning which uncertain variables are important (with
relatively few function evaluations), or compute full cumulative or
complementary cumulative response functions (with additional
computational effort).  Since they rely on gradient calculations to
compute local optima (most probable points of failure), they scale
well for increasing numbers of random variables, but issues with
nonsmooth, discontinuous, and multimodal response functions are
relevant concerns. In addition, even if there is a single MPP and it
is calculated accurately, first-order and second-order integrations
may fail to accurately capture the shape of the failure domain.  In
these cases, adaptive importance sampling around the MPP can be
helpful.  Overall, local reliability methods should be used with some
care and their accuracy should be verified whenever possible.

An effective alternative to local reliability analysis when confronted
with nonsmooth, multimodal, and/or highly nonlinear response functions
is efficient global reliability analysis (EGRA).  This technique
employs Gaussian process global surrogate models to accurately resolve
the failure domain and then employs multimodal adaptive importance
sampling to resolve the probabilities.  For relatively low dimensional
problems (i.e, on the order of 10 variables), this method displays the
efficiency of local reliability analysis with the accuracy of
exhaustive sampling.  While extremely promising, this method is still
relatively new and is the subject of ongoing refinements as we deploy
it to additional applications.

The next class of UQ methods available in DAKOTA is comprised of
stochastic expansion methods (polynomial chaos and stochastic
collocation), which are general purpose techniques provided that the
response functions possess finite second order moments. Further, these
methods approximate the full random process/field and capture the
underlying functional relationship between a key response metric and
its random variables, rather than just approximating statistics such
as mean and standard deviation. This class of methods parallels
traditional variational methods in mechanics; in that vein, efforts
are underway to compute rigorous error bounds of the approximations
produced by the methods. Another strength of these methods is their
potential use in a multiphysics environment as a means to propagate
the uncertainty through a series of simulations while retaining as
much information as possible at each stage of the analysis.  The
current challenge in the development of these methods, as for other
global surrogate-based methods, is effective scaling for large numbers
of random variables.  Recent advances in adaptive sparse grid methods 
address some of the scaling issues for stochastic expansion.

The final class of UQ methods available in DAKOTA are focused on
epistemic uncertainties, or uncertainties resulting from a lack of
knowledge.  In these problems, the assignment of input probability
distributions when data is sparse can be somewhat suspect.  One
approach to handling epistemic uncertainties is Dempster-Shafer theory
of evidence (DAKOTA methods \texttt{local\_evidence} and 
\texttt{global\_evidence}). Another method is pure interval 
analysis (\texttt{local\_interval\_est} and 
\texttt{global\_interval\_est}), where intervals on inputs are 
mapped to intervals on outputs using optimization methods 

For problems with a mixture of epistemic and aleatoric uncertainties, 
it is desirable to segregate the two uncertainty types within a nested 
analysis, allowing stronger probabilistic inferences for the portion
of the problem where they are appropriate.  In this nested approach, an outer 
epistemic level selects realizations of epistemic parameters (augmented
variables) and/or realizations of random variable distribution
parameters (inserted variables) from intervals.  These realizations
define the probability distributions for an inner aleatoric level
performing probabilistic analyses.  In the case where the outer loop
is an interval propagation approach (\texttt{local\_interval\_est} or 
\texttt{global\_interval\_est}), the nested approach is
known as second-order probability (see also Section~\ref{models:nested})
and the study generates a family of CDF/CCDF respresentations known as a 
``horse tail'' plot.  In the case where the outer loop is an 
evidence-based approach (\texttt{local\_evidence} or 
\texttt{global\_evidence}), the approach generates epistemic belief 
and plausibility bounds on aleatory statistics.

The recommendations for UQ methods are summarized in Table~\ref{usage:guideuq}.

\begin{table}
\centering
\caption{Guidelines for UQ method selection.} \label{usage:guideuq}\vspace{2mm}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Method} & \textbf{Desired Problem} & \textbf{Applicable Methods} \\
\textbf{Classification} & \textbf{Characteristics} & \\
\hline
Sampling & nonsmooth, multimodal response functions;       & sampling 
(Monte Carlo or LHS) \\
         & response evaluations are relatively inexpensive & \\
\hline
Local       & smooth, unimodal response functions; & local\_reliability
(MV, AMV/AMV$^2$,\\
reliability & larger sets of random variables & AMV+/AMV$^2$+, TANA, 
FORM/SORM) \\
\hline
Global      & nonsmooth, multimodal response functions; & 
global\_reliability \\
reliability & low dimensional     & \\
\hline
Stochastic & nonsmooth, multimodal response functions; & 
polynomial\_chaos, \\
expansions & low dimensional; capture of functional    & 
stoch\_collocation\\
           & form useful for subsequent analyses       & \\
\hline
Epistemic & uncertainties are poorly characterized &
interval: local\_interval\_est, \\
 & & global\_interval\_est, sampling; \\
 & & BPA: local\_evidence, \\
 & & global\_evidence \\
\hline
Mixed UQ  & some uncertainties are poorly characterized &
nested UQ (second-order probability, \\
 & & mixed evidence) with epistemic outer loop \\
 & & and aleatory inner loop, sampling \\
\hline
\end{tabular}
\end{table}

\section{Parameter Study/DOE/DACE/Sampling Method Selection}\label{usage:ps}

Parameter studies, classical design of experiments (DOE),
design/analysis of computer experiments (DACE), and sampling methods
share the purpose of exploring the parameter space.  If directed
studies with a defined structure are desired, then parameter study
methods (see Chapter~\ref{ps}) are recommended. For example, a quick
assessment of the smoothness of a response function is best addressed
with a vector or centered parameter study. Also, performing local
sensitivity analysis is best addressed with these methods. If,
however, a global space-filling set of samples is desired, then the
DOE, DACE, and sampling methods are recommended (see
Chapter~\ref{dace}).  These techniques are useful for scatter plot and
variance analysis as well as surrogate model construction. The
distinction between DOE and DACE methods is that the former are
intended for physical experiments containing an element of
nonrepeatability (and therefore tend to place samples at the extreme
parameter vertices), whereas the latter are intended for repeatable
computer experiments and are more space-filling in nature. The
distinction between DOE/DACE and sampling is drawn based on the
distributions of the parameters.  DOE/DACE methods typically assume
uniform distributions, whereas the sampling approaches in DAKOTA
support a broad range of probability distributions. To use
\texttt{sampling} in a design of experiments mode (as opposed to
an uncertainty quantification mode), the \texttt{all\_variables} flag
should be included in the method specification of the DAKOTA input
file.

These method selection recommendations are summarized in
Table~\ref{usage:guidepsdace}.

\begin{table}
\centering
\caption{Guidelines for selection of parameter study, DOE, DACE, and
sampling methods.}
\label{usage:guidepsdace}\vspace{2mm}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Method} & \textbf{Applications} & \textbf{Applicable Methods} \\
\textbf{Classification} & & \\
\hline
parameter study & sensitivity analysis,    & centered\_parameter\_study, \\
                & directed parameter space & list\_parameter\_study, \\
                & investigations           & multidim\_parameter\_study, \\
                &                          & vector\_parameter\_study \\
\hline
classical design & physical experiments    & dace (box\_behnken, \\
of experiments   & (parameters are         & central\_composite) \\
                 & uniformly distributed)  & \\
                 &                         & \\
\hline
design of computer & variance analysis,     & dace (grid, random,
                                              oas, lhs, oa\_lhs), \\
experiments        & space filling designs  & fsu\_quasi\_mc (halton, 
                                              hammersley), \\
                   & (parameters are        & fsu\_cvt, psuade\_moat \\
                   & uniformly distributed) & \\
\hline
sampling           & space filling designs      & sampling
                                                  (Monte Carlo or LHS) \\
                   & (parameters have general   & with all\_variables flag \\
                   & probability distributions) & \\
\hline
\end{tabular}
\end{table}

\section{Surrogate Model Selection}\label{usage:model}
Surrogate models provide an approximation to an original, high 
fidelity "truth" model. These are useful both directly, to interpolate
or extrapolate data without running the possibly expensive simulator, 
and as a way to reduce the cost of optimization (see Chapter \ref{sbm} 
for more details) and uncertainty quantification (see Chapter \ref{uq}).
This section is intended to provide some rudimentary advice regarding 
when to choose which model.  

\newpage
{\large Recommendations:}
\begin{itemize}
\item For Surrogate Based Local Optimization, i.e. the 
      \texttt{surrogate\_based\_local} method, with a trust region, either
      \texttt{surrogate} \texttt{local} \texttt{taylor\_series} or
      \texttt{surrogate} \texttt{multipoint} \texttt{tana} will probably 
      work best.  If for some reason you wish or need to use a global 
      surrogate (not recommended) then the best of these options is likely 
      to be either 
      \texttt{surrogate} \texttt{global} 
      \texttt{gaussian\_process} \texttt{surfpack} or
      \texttt{surrogate} \texttt{global} \texttt{moving\_least\_squares}.
\item For Efficient Global Optimization (EGO), i.e. the 
      \texttt{efficient\_global} method, the default\\
      \texttt{gaussian\_process} \texttt{surfpack}  
      is likely to find a more optimal value and/or use fewer true 
      function evaluations than the alternative,
      \texttt{gaussian\_process} \texttt{dakota}.  However, the 
      \texttt{surfpack} version will likely take more time to build 
      than the \texttt{dakota} version.  Note that currently the 
      \texttt{use\_derivatives} keyword is not recommended for use with
      EGO based methods.
\item For EGO based global interval estimation (EGIE), i.e. the 
      \texttt{global\_interval\_est} \texttt{ego} method, 
      the default \texttt{gaussian\_process} \texttt{surfpack} will
      likely work better than the alternative \texttt{gaussian\_process} 
      \texttt{dakota}.
\item For Efficient Global Reliability Analysis (EGRA), i.e. the 
      \texttt{global\_reliability} method the \texttt{surfpack} and 
      \texttt{dakota} versions of the gaussian process tend to give 
      similar answers with the \texttt{dakota} version tending to use
      fewer true function evaluations.  Since this is based on EGO, it
      is likely that the default \texttt{surfpack} version is more 
      accurate, although this has not been rigorously demonstrated.
\item For EGO based Dempster-Shafer Theory of Evidence, i.e. the 
      \texttt{global\_evidence} \texttt{ego} method, the default
      \texttt{gaussian\_process} \texttt{surfpack} will often use
      significantly fewer true function evaluations than the 
      alternative \texttt{gaussian\_process} \texttt{dakota}.
\item When using a global surrogate to extrapolate, either the
      \texttt{gaussian\_process} \texttt{surfpack} or 
      \texttt{polynomial} \texttt{quadratic} or 
      \texttt{polynomial} \texttt{cubic} is recommended.
\item When there is over roughly two or three thousand data points 
      and you wish to interpolate (or approximately interpolate) then 
      a Taylor series, Radial Basis Function Network, or Moving Least
      Squares fit is recommended.  The only reason that the 
      \texttt{gaussian\_process} \texttt{surfpack} model is not 
      recommended is that it can take a considerable amount of time
      to construct when the number of data points is very large.  Use 
      of the third party MARS package included in DAKOTA is generally 
      discouraged.
\item In other situations that call for a global surrogate, the 
      \texttt{gaussian\_process} \texttt{surfpack} is generally 
      recommended.  The \texttt{use\_derivatives} keyword will 
      only be useful if accurate and an inexpensive derivatives 
      are available. Finite difference derivatives are disqualified 
      on both counts.  However, derivatives generated by analytical,
      automatic differentiation, or continuous adjoint techniques
      can be appropriate.  Currently, first order derivatives, i.e.
      gradients, are the highest order derivatives that can be used
      to construct the \texttt{gaussian\_process} \texttt{surfpack}
      model; Hessians will not be used even if they are available.
\end{itemize}
      