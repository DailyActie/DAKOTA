\chapter{DAKOTA Capability Overview}\label{capabilities}

\section{Purpose}\label{capabilities:purpose}

This chapter provides a brief overview of DAKOTA's capabilities.
Emerging capabilities in solution verification and Bayesian
calibration/uncertainty quantification are not yet documented here.
Additional details and example problems are provided in subsequent
chapters in this manual.

\section{Parameter Study Methods}\label{capabilities:parameter}

Parameter studies are often performed to explore the effect of
parametric changes within simulation models. DAKOTA users may select
from four parameter study methods.

\textbf{Multidimensional}: Forms a regular lattice or grid in an
n-dimensional parameter space, where the user specifies the number of
intervals used for each parameter.

\textbf{Vector}: Performs a parameter study along a line between any two
points in an n-dimensional parameter space, where the user specifies
the number of steps used in the study.

\textbf{Centered}: Given a point in an n-dimensional parameter space,
this method evaluates nearby points along the coordinate axes of the
parameter space. The user selects the number of steps and the step
size.

\textbf{List}: The user supplies a list of points in an n-dimensional
space where DAKOTA will evaluate response data from the simulation
code.

Additional information on these methods is provided in Chapter~\ref{ps}.

\section{Design of Experiments}\label{capabilities:sampling}

Design of experiments are often used to explore the parameter space of
an engineering design problem, for example to perform global
sensitivity analysis.  In design of experiments, especially design of
computer experiments, one wants to generate input points that provide
good coverage of the input parameter space.  There is significant
overlap between design of experiments and sampling methods, and both
techniques can yield similar results about response function behavior
and the relative importance of the input variables.  We consider
design of experiment methods to generate sets of uniform random
variables on the interval $[0,1]$, with the goal of characterizing the
behavior of the response functions over the input parameter ranges of
interest. Uncertainty quantification, in contrast, involves
characterizing the uncertain input variables with probability
distributions such as normal, Weibull, triangular, etc., sampling from
the input distributions, and propagating the input uncertainties to
obtain a cumulative distribution function on the output or system
response.  We typically use the Latin Hypercube Sampling software
(also developed at Sandia) for generating samples on input
distributions used in uncertainty quantification.  LHS is explained in
more detail in the subsequent section ~\ref{capabilities:uncertainty}.
Two software packages are available in DAKOTA for design of computer
experiments, DDACE (developed at Sandia Labs) and FSUDACE (developed
at Florida State University).

\textbf{DDACE (Distributed Design and Analysis of Computer Experiments)}:
The DACE package includes both stochastic sampling methods and
classical design of experiments methods~\cite{TonXX}. The stochastic
methods are Monte Carlo (random) sampling, Latin Hypercube sampling,
orthogonal array sampling, and orthogonal array-latin hypercube
sampling. The orthogonal array sampling allows for the calculation
of main effects. The DDACE package currently supports variables that have
either normal or uniform distributions.  However, only the uniform
distribution is available in the DAKOTA interface to DDACE. The
classical design of experiments methods in DDACE are central composite
design (CCD) and Box-Behnken (BB) sampling. A grid-based sampling
method also is available. DDACE is available under a GNU Lesser
General Public License and is distributed with DAKOTA.

\textbf{FSUDace (Florida State University Design and Analysis of
Computer Experiments)}: The FSUDace package provides quasi-Monte Carlo
sampling (Halton and Hammersley) and Centroidal Voronoi Tessellation
(CVT) methods.  The quasi-Monte Carlo and CVT methods are
designed with the goal of low discrepancy. Discrepancy refers to the
non-uniformity of the sample points within the unit hypercube. Low
discrepancy sequences tend to cover the unit hypercube reasonably
uniformly. Quasi-Monte Carlo methods produce low discrepancy
sequences, especially if one is interested in the uniformity of
projections of the point sets onto lower dimensional faces of the
hypercube. CVT does very well volumetrically: it spaces
the points fairly equally throughout the space, so that the points
cover the region and are isotropically distributed with no directional
bias in the point placement.
FSUDace is available under a GNU Lesser General Public
License and is distributed with DAKOTA.

\textbf{PSUADE (Problem Solving Environment for Uncertainty Analysis
and Design Exploration)}: PSUADE is a Lawrence Livermore National
Laboratory tool for metamodeling, sensitivity analysis, uncertainty
quantification, and optimization.  Its features include non-intrusive
and parallel function evaluations, sampling and analysis methods, an
integrated design and analysis framework, global optimization,
numerical integration, response surfaces (MARS and higher order
regressions), graphical output with Pgplot or Matlab, and fault
tolerance~\cite{Ton05}. DAKOTA includes a prototype interface to its MOAT
sampling method, a valuable tool for global sensitivity analysis.

Additional information on these methods is provided in Chapter~\ref{dace}.

\section{Uncertainty Quantification}\label{capabilities:uncertainty}

Uncertainty quantification methods (also referred to as
nondeterministic analysis methods) involve the computation of
probabilistic information about response functions based on sets of
simulations taken from the specified probability distributions for
uncertain input parameters. Put another way, these methods perform a
forward uncertainty propagation in which probability information for
input parameters is mapped to probability information for output
response functions. We usually distinguish the UQ methods in terms of 
their capability to handle aleatory or epistemic uncertainty. 
Input uncertainties may be characterized as either aleatory
uncertainties, which are irreducible variabilities inherent in nature,
or epistemic uncertainties, which are reducible uncertainties
resulting from a lack of knowledge.  Since sufficient data is
generally available for aleatory uncertainties, probabilistic methods
are commonly used for computing response distribution statistics based
on input probability distribution specifications.  Conversely, for
epistemic uncertainties, data is generally sparse, making the use of
probability theory questionable and leading to non-probabilistic
methods based on interval specifications.
The aleatory UQ methods in DAKOTA include various
sampling-based approaches (e.g., Monte Carlo and Latin Hypercube
sampling), local and global reliability methods, and stochastic expansion
approaches.  The epistemic UQ methods include interval 
analysis and Dempster-Shafer evidence theory.

\textbf{LHS (Latin Hypercube Sampling)}: This package provides both
Monte Carlo (random) sampling and Latin Hypercube sampling methods,
which can be used with probabilistic variables in DAKOTA that have the
following distributions: normal, lognormal, uniform, loguniform,
triangular, exponential, beta, gamma, gumbel, frechet, weibull, poisson, 
binomial, negative binomial, geometric, hypergeometric, and
user-supplied histograms. In addition, LHS accounts for correlations
among the variables~\cite{Ima84}, which can be used to accommodate a
user-supplied correlation matrix or to minimize correlation when a
correlation matrix is not supplied. The LHS package currently serves
two purposes: (1) it can be used for uncertainty quantification by
sampling over uncertain variables characterized by probability
distributions, or (2) it can be used in a DACE mode in which any
design and state variables are treated as having uniform distributions
(see the \texttt{all\_variables} flag in the DAKOTA Reference
Manual~\cite{RefMan}). The LHS package historically came in two
versions: ``old'' (circa 1980) and ``new'' (circa 1998), but presently
only the latter is supported in DAKOTA, requiring a Fortran 90
compiler.  This ``new'' LHS is available under a separate GNU Lesser
General Public License and is distributed with DAKOTA.  In addition 
to a standard sampling study, we support the capability to perform 
``incremental'' LHS, where a user can specify an initial LHS study 
of N samples, and then re-run an additional incremental study which 
will double the number of samples (to 2N, with the first N being 
carried from the initial study).  The full incremental sample of 
size 2N is also a Latin Hypercube, with proper stratification and 
correlation.  Finally, DAKOTA offers preliminary support for 
importance sampling using LHS, specified with \texttt{importance}.

\textbf{Reliability Methods}: This suite of methods includes both
local and global reliability methods.  Local methods include first-
and second-order versions of the Mean Value method (MVFOSM and MVSOSM)
and a variety of most probable point (MPP) search methods, including
the Advanced Mean Value method (AMV and AMV$^2$), the iterated
Advanced Mean Value method (AMV+ and AMV$^2$+), the Two-point Adaptive
Nonlinearity Approximation method (TANA-3), and the traditional First
Order and Second Order Reliability Methods (FORM and
SORM)~\cite{Hal00}.  Each of the MPP search techniques solve local
optimization problems in order to locate the MPP, which is then used
as the point about which approximate probabilities are integrated
(using first- or second-order integrations in combination with
refinements based on importance sampling). Reliability mappings may
involve computing reliability and probability levels for prescribed
response levels (forward reliability analysis, commonly known as the
reliability index approach or RIA) or computing response levels for
prescribed reliability and probability levels (inverse reliability
analysis, commonly known as the performance measure approach or PMA).
Approximation-based MPP search methods (AMV, AMV$^2$, AMV+, AMV$^2$+,
and TANA) may be applied in either x-space or u-space, and mappings
may involve either cumulative or complementary cumulative distribution
functions.  Global reliability methods are designed to handle
nonsmooth and multimodal failure surfaces, by creating global
approximations based on Gaussian process models.  They accurately
resolve a particular contour of a response function and then estimate
probabilities using multimodal adaptive importance sampling.

\textbf{Stochastic Expansion Methods}: The objective of these
techniques is to characterize the response of systems whose governing
equations involve stochastic coefficients. The development of these
techniques mirrors that of deterministic finite element analysis
utilizing the notions of projection, orthogonality, and weak
convergence~\cite{Gha99},~\cite{Gha91}.  Rather than estimating point
probabilities, they form an approximation to the functional
relationship between response functions and their random inputs, which
provides a more complete uncertainty representation for use in
multi-code simulations.  Expansion methods include the Wiener-Askey
generalized polynomial chaos expansion (PCE), which employs a family
of multivariate orthogonal polynomials that are well matched to
particular input probability distributions, and stochastic collocation
(SC), which employs multivariate Lagrange interpolation polynomials.
For PCE, expansion coefficients may be evaluated using a spectral
projection approach (based on sampling, quadrature, or sparse grid
methods for integration) or a point collocation approach (based on
linear regression).  For SC, interpolants may be formed over
tensor-product quadrature grids or Smolyak sparse grids.  Both methods
provide analytic response moments; however, CDF/CCDF probabilities are
evaluated by sampling on the expansion.

\textbf{Interval Analysis}: Interval analysis is often used to model 
epistemic uncertainty. In interval analysis, one assumes that nothing 
is known about an epistemic uncertain variable except that its value lies 
somewhere within an interval.  In this situation, it is NOT 
assumed that the value has a uniform probability of occurring 
within the interval.  Instead, the interpretation is that 
any value within the interval is a possible value or a potential 
realization of that variable.  In interval analysis, the 
uncertainty quantification problem is one of determining the 
resulting bounds on the output (defining the output interval) 
given interval bounds on the inputs. Again, any output response 
that falls within the output interval is a possible output 
with no frequency information assigned to it.

We have the capability to perform interval analysis using either
global or local methods. In the global approach, one uses either a 
global optimization method (based on a Gaussian process surrogate model)
or a sampling method to assess the bounds. The
local method uses gradient information in a derivative-based 
optimization approach, using either SQP (sequential quadratic 
programming) or a NIP (nonlinear interior point) method to obtain bounds. 
 
\textbf{Dempster-Shafer Theory of Evidence}: The objective of Evidence
theory is to model the effects of epistemic uncertainties.  Epistemic
uncertainty refers to the situation where one does not know enough
to specify a probability distribution on a variable.  Sometimes epistemic
uncertainty is referred to as subjective, reducible, or lack of knowledge
uncertainty.  In contrast, aleatory uncertainty refers to the situation
where one does have enough information to specify a probability distribution.
In Dempster-Shafer theory of evidence, the uncertain input variables
are modeled as sets of intervals.  The user assigns a basic probability
assignment (BPA) to each interval, indicating how likely it is that the
uncertain input falls within the interval.  The intervals may be
overlapping, contiguous, or have gaps.  The intervals and their associated
BPAs are then propagated through the simulation to obtain cumulative
distribution functions on belief and plausibility.  Belief is the lower
bound on a probability estimate that is consistent with the evidence, and
plausibility is the upper bound on a probability estimate that is consistent
with the evidence. In addition to the full evidence theory structure, 
we have a simplified capability for users wanting to perform pure 
interval analysis (e.g. what is the interval on the output given 
intervals on the input) using either global or local optimization methods. 
Interval analysis is often used to model epistemic variables in 
nested analyses, where probability theory is used to model aleatory variables.

Additional information on these methods is provided in Chapter~\ref{uq}.

\section{Optimization}\label{capabilities:optimization1}

Several optimization software packages have been integrated with
DAKOTA. These include freely-available software packages developed by
research groups external to Sandia Labs, Sandia-developed software
that has been released to the public under GNU licenses, and
commercially-developed software. These optimization software packages
provide the DAKOTA user with access to well-tested, proven methods for
use in engineering design applications, as well as access to some of
the newest developments in optimization algorithm research.

\textbf{HOPSPACK}: is a library that enables the implementation of
hybrid optimization algorithms~\cite{Plantenga2009}.  Currently the
only method exposed is an asynchronous implementation of generating
set search known as asynchronous parallel pattern search
(APPS)~\cite{GrKo06}.  It can handle unconstrained problems as well as
those with bound constraints, linear constraints~\cite{GrKoLe08}, and
general nonlinear constraints~\cite{GrKo07}.  APPS was previously
integrated with DAKOTA via the APPSPACK software, but is now
integrated through it's successor (HOPSPACK).  HOPSPACK is available
to the public under the GNU LGPL and the source code is included with
DAKOTA (web page: \url{https://software.sandia.gov/trac/hopspack}).

\textbf{COLINY}: Methods for nongradient-based local and global
optimization which utilize the Common Optimization Library INterface
(COLIN). %This algorithm library supersedes the SGOPT library. 
COLINY currently includes evolutionary algorithms (including several
genetic algorithms and Evolutionary Pattern Search), simple pattern
search, Monte Carlo sampling, and the DIRECT and Solis-Wets
algorithms. COLINY also include interfaces to third-party optimizer
COBYLA2.  This software is available to the public under a GNU Lesser
General Public License (LGPL) through ACRO (A Common Repository for
Optimizers) and the source code for COLINY is included with DAKOTA
(web page: \url{http://software.sandia.gov/trac/acro}).

\textbf{CONMIN (CONstrained MINimization)}: Methods for gradient-based
constrained and unconstrained optimization~\cite{Van78}. The constrained
optimization algorithm is the method of feasible directions (MFD) and
the unconstrained optimization algorithm is the Fletcher-Reeves
conjugate gradient (CG) method. This software is freely available to
the public from NASA, and the CONMIN source code is included with
DAKOTA.

\textbf{DOT (Design Optimization Tools)}: Methods for gradient-based
optimization for constrained and unconstrained optimization problems
~\cite{Van95}. The algorithms available for constrained optimization are
modified-MFD, SQP, and sequential linear programming (SLP). The
algorithms available for unconstrained optimization are the
Fletcher-Reeves CG method and the Broyden-Fletcher-Goldfarb-Shanno
(BFGS) quasi-Newton technique. DOT is a commercial software product of
Vanderplaats Research and Development, Inc. (web page:
\url{http://www.vrand.com}). Sandia National Laboratories and Los
Alamos National Laboratory have limited seats for DOT. \emph{Other
users may obtain their own copy of DOT and compile it with the DAKOTA
source code by following the steps given in the file {\tt Dakota/INSTALL}.}

\textbf{JEGA}: provides SOGA and MOGA (single- and multi-objective
genetic algorithms) optimization methods. The SOGA method provides a
basic GA optimization capability that uses many of the same software
elements as the MOGA method. The MOGA package allows for the
formulation of multiobjective optimization problems without the need
to specify weights on the various objective function values. The MOGA
method directly identifies non-dominated design points that lie on the
Pareto front through tailoring of its genetic search operators.  The
advantage of the MOGA method versus conventional multiobjective
optimization with weight factors (see
Section~\ref{capabilities:additional}), is that MOGA finds points
along the entire Pareto front whereas the multiobjective optimization
method produces only a single point on the Pareto front. The advantage
of the MOGA method versus the Pareto-set optimization strategy (see
Section~\ref{capabilities:optimization2}) is that MOGA is better able
to find points on the Pareto front when the Pareto front is
nonconvex. However, the use of a GA search method in MOGA causes the
MOGA method to be much more computationally expensive than
conventional multiobjective optimization using weight factors.

%\textbf{MOOCHO (Multifunctional Object-Oriented arCHitecture for
%Optimization)}: formerly known as rSQP++, MOOCHO provides both
%general-purpose gradient-based algorithms for nested analysis and
%design (NAND) and large-scale gradient-based optimization algorithms
%for simultaneous analysis and design (SAND). This software is not yet
%distributed with DAKOTA.

\textbf{NCSUOpt}: Nongradient-based optimizers from North Carolina
State University, including DIRECT and, eventually, impicit filtering
(web site: \url{http://www4.ncsu.edu/~ctk/matlab_darts.html}).  We
currently incorporate only an implementation of the DIRECT (DIviding
RECTangles) algorithm~\cite{Gab01}.  While this is somewhat redundant
with DIRECT supplied by Coliny, we have found that NCCSU DIRECT
performs better in some cases, and presently we maintain both versions
in DAKOTA.

\textbf{NLPQLP}: Methods for gradient-based constrained and
unconstrained optimization problems using a sequential quadratic
programming (SQP) algorithm~\cite{Sch04}.  NLPQLP is a commercial
software product of Prof. Klaus Schittkowski (web site:
\url{http://www.uni-bayreuth.de/departments/math/~kschittkowski/nlpqlp20.htm}).
\emph{Users may obtain their own copy of NLPQLP and compile it with the
DAKOTA source code by following the steps given in the file {\tt Dakota/INSTALL}.}

\textbf{NPSOL}: Methods for gradient-based constrained and
unconstrained optimization problems using a sequential quadratic
programming (SQP) algorithm~\cite{Gil86}. NPSOL is a commercial software
product of Stanford University (web site: www.sbsi-sol-optimize.com).
Sandia National Laboratories, Lawrence Livermore National Laboratory,
and Los Alamos National Laboratory all have site licenses for NPSOL.
\emph{Other users may obtain their own copy of NPSOL and compile it
with the DAKOTA source code by following the steps given in the file
{\tt Dakota/INSTALL}.}

\textbf{OPT++}: Methods for gradient-based and nongradient-based
optimization of unconstrained, bound-constrained, and nonlinearly
constrained optimization problems~\cite{MeOlHoWi07}. OPT++ includes a
variety of Newton-based methods (quasi-Newton, finite-difference
Newton, Gauss-Newton, and full-Newton), as well as the Polak-Ribeire
CG method and the parallel direct search (PDS) method. OPT++ now
contains a nonlinear interior point algorithm for handling general
constraints.  OPT++ is available to the public under the GNU LGPL and
the source code is included with DAKOTA (web page:
\url{http://csmr.ca.sandia.gov/opt++}).

\textbf{PICO (Parallel Integer Combinatorial Optimization)}: PICO's
branch-and-bound algorithm can be applied to nonlinear optimization
problems involving discrete variables or a combination of continuous
and discrete variables~\cite{Eck01}. The discrete variables must be
noncategorical (see Section~\ref{variables:design:ddv}).  PICO is
available to the public under the GNU LGPL (web page:
\url{http://software.sandia.gov/trac/acro/wiki/Packages}) and the source code is included
with DAKOTA as part of the Acro package.  \emph{Notes: (1) PICO's linear
programming solvers are not included with DAKOTA, (2) PICO is being
migrated into COLINY and is not operational in DAKOTA 5.1.}

%\textbf{SGOPT (Stochastic Global OPTimization)}: Access to this
%library within DAKOTA has been deprecated; the methods have been
%migrated to the COLINY library.

Additional information on these methods is provided in Chapter~\ref{opt},
as is a facility for using other solvers made available to DAKOTA
via shared libraries.

\section{Additional Optimization Capabilities}\label{capabilities:additional}

The optimization software packages described above provide algorithms
to handle a wide variety of optimization problems. This includes
algorithms for constrained and unconstrained optimization, as well as
algorithms for gradient-based and nongradient-based
optimization. Listed below are additional optimization capabilities
that are available in DAKOTA.

\textbf{Multiobjective Optimization}:
There are three capabilities for multiobjective optimization in
DAKOTA.  First, there is the MOGA capability described previously in
Section~\ref{capabilities:optimization1}.  This is a specialized
algorithm capability.  The second capability involves the use of
response data transformations to recast a multiobjective problem as a
single-objective problem.  Currently, DAKOTA supports the weighting
factor approach for this transformation, in which a composite
objective function is constructed from a set of individual objective
functions using a user-specified set of weighting factors.  This
approach is optimization algorithm independent, in that it works with
any of the optimization methods listed in
Section~\ref{capabilities:optimization1}.  Constraints are not
affected by the weighting factor mapping; therefore, both constrained
and unconstrained multiobjective optimization problems can be
formulated and solved with DAKOTA, assuming selection of an
appropriate constrained or unconstrained single-objective optimization
algorithm.  Future multiobjective response data transformations for
goal programming, normal boundary intersection, etc. are planned.  The
third capability is the Pareto-set optimization strategy described in
Section~\ref{capabilities:optimization2}.  This capability also
utilizes the multiobjective response data transformations to allow
optimization algorithm independence; however, it builds upon the basic
approach by computing sets of optima in order to generate a Pareto
trade-off surface.

%\textbf{Simultaneous Analysis and Design (SAND)}: In SAND, one
%converges the optimization process at the same time as converging a
%nonlinear simulation code. In this approach, the solution of the
%simulation code (often a system of ordinary or partial differential
%equations) is posed as a set of equality constraints in the
%optimization problem and these equality constraints are only satisfied
%by the optimizer in the limit. This formulation necessitates a close
%coupling between DAKOTA and the simulation code so that the internal
%vectors and matrices from the simulation code (in particular, the
%residual vector and its state and design Jacobian matrices) are
%available to the SAND optimizer. This approach has the potential to
%reduce the cost of optimization significantly since the nonlinear
%simulation is only converged once, instead of on every function
%evaluation. The drawback is that this approach requires substantial
%software modifications to the simulation code; something that can be
%impractical in some cases and impossible in others. A new SAND
%capability employing the MOOCHO library is under development that will
%intrusively couple DAKOTA with multiphysics simulation frameworks
%under development at Sandia.

\textbf{User-Specified or Automatic Scaling}: Some optimization
algorithms are sensitive to the relative scaling of problem inputs and
outputs.  With any optimizer or least squares solver, user-specified
(and in some cases automatic or logarithmic) scaling may be applied to
continuous design variables, responses (objectives or residuals),
nonlinear inequality and equality constraints, and/or linear
inequality and equality constraints.

Additional information on these capabilities is provided in Chapter~\ref{opt}.

\section{Nonlinear Least Squares for Parameter Estimation}\label{capabilities:nonlinear}

Nonlinear least squares methods are optimization algorithms which
exploit the special structure of a least squares objective function
(see Section~\ref{introduction:background:nonlinear}). These problems
commonly arise in parameter estimation and test/analysis
reconciliation. In practice, least squares solvers will tend to
converge more rapidly than general-purpose optimization algorithms
when the residual terms in the least squares formulation tend towards
zero at the solution. Least squares solvers may experience difficulty
when the residuals at the solution are significant, although
experience has shown that the NL2SOL method can handle some problems
that are highly nonlinear and have nonzero residuals at the solution.

\textbf{NL2SOL}: The NL2SOL algorithm~\cite{Den81} uses a secant-based
algorithm to solve least-squares problems. In practice, it is more
robust to nonlinear functions and nonzero residuals than conventional
Gauss-Newton algorithms.

\textbf{Gauss-Newton}: DAKOTA's Gauss-Newton algorithm utilizes the
Hessian approximation described in
Section~\ref{introduction:background:nonlinear}. The exact objective
function value, exact objective function gradient, and the approximate
objective function Hessian are defined from the least squares term
values and gradients and are passed to the full-Newton optimizer from
the OPT++ software package. As for all of the Newton-based
optimization algorithms in OPT++, unconstrained, bound-constrained,
and generally-constrained problems are supported. However, for the
generally-constrained case, a derivative order mismatch exists in that
the nonlinear interior point full Newton algorithm will require
second-order information for the nonlinear constraints whereas the
Gauss-Newton approximation only requires first order information for
the least squares terms.

\textbf{NLSSOL}: The NLSSOL algorithm is a commercial software product
of Stanford University (web site: \url{http://www.sbsi-sol-optimize.com})
that is bundled with current versions of the NPSOL library. It uses an
SQP-based approach to solve generally-constrained nonlinear least
squares problems. It periodically employs the Gauss-Newton Hessian
approximation to accelerate the search. It requires only first-order
information for the least squares terms and nonlinear constraints.
Sandia National Laboratories, Lawrence Livermore National Laboratory,
and Los Alamos National Laboratory all have site licenses for NLSSOL.
\emph{Other users may obtain their own copy of NLSSOL and compile it with
the DAKOTA source code by following the NPSOL installation steps given
in the file {\tt Dakota/INSTALL}.}

Additional information on these methods is provided in Chapter~\ref{nls}.

\section{Surrogate-Based Minimization}\label{capabilities:sbm}

\textbf{Surrogate-Based Local Minimization}: This method combines
the design of experiments methods, surrogate models, and optimization
capabilities of DAKOTA.
%The SBO strategy is particularly effective on
%real-world engineering design problems that contain nonsmooth features
%(e.g., slope discontinuities, multiple local minima) where
%gradient-based optimization methods often have trouble.
In SBO, the optimization algorithm operates on a surrogate model
instead of directly operating on the computationally expensive
simulation model.  The surrogate model can be formed from data fitting
methods (local, multipoint, or global), from a lower fidelity version
of the computational model, or from a mathematically-generated
reduced-order model (see Section~\ref{capabilities:surrogate}). For
each of these surrogate model types, the SBO algorithm periodically
validates the progress using the surrogate model against the original
high-fidelity model.  The SBO strategy in DAKOTA can be configured to
employ heuristic rules (less expensive) or to be provably convergent
to the optimum of the original model (more expensive). The development
of SBO strategies is an area of active research in the DAKOTA project.

\textbf{Surrogate-Based Global Minimization}: Similar to surrogate-based
local minimization, this method combines design of experiments and
surrogate modeling with optimization.  However, rather than employing
trust region model management to localize and control the extent of
the approximation in order to ensure convergence to a local minimum,
the surrogate-based global method sequentially refines the full range
of a global approximation using global optimizers.

\textbf{Efficient Global Minimization}: Methods for nongradient-based
constrained and unconstrained optimization and nonlinear least squares
based on Gaussian process models.  This approach uses an expected
improvement function derived from the expected value and variance
estimators in Gaussian process models, and is designed to balance
exploitation of regions with good solutions and exploration of regions
with limited data.~\cite{Jon98}

Additional information on these methods is provided in Chapter~\ref{sbm}.

\section{Optimization Strategies}\label{capabilities:optimization2}

Due to the flexibility of DAKOTA's object-oriented design, it is
relatively easy to create algorithms that combine several of DAKOTA's
capabilities. These algorithms are referred to as \emph{strategies}:

\textbf{Multilevel Hybrid Optimization}: This strategy allows the user to
specify a sequence of optimization methods, with the results from one
method providing the starting point for the next method in the
sequence. An example which is useful in many engineering design
problems involves the use of a nongradient-based global optimization
method (e.g., genetic algorithm) to identify a promising region of the
parameter space, which feeds its results into a gradient-based method
(quasi-Newton, SQP, etc.) to perform an efficient local search for the
optimum point.

\textbf{Multistart Local Optimization}: This strategy uses many local
optimization runs (often gradient-based), each of which is started
from a different initial point in the parameter space. This is an
attractive strategy in situations where multiple local optima are
known to exist or may potentially exist in the parameter space. This
approach combines the efficiency of local optimization methods with
the parameter space coverage of a global stratification technique.

\textbf{Pareto-Set Optimization}: The Pareto-set optimization strategy
allows the user to specify different sets of weights for the
individual objective functions in a multiobjective optimization
problem. DAKOTA executes each of these weighting sets as a separate
optimization problem, serially or in parallel, and then outputs the
set of optimal designs which define the Pareto set. Pareto set
information can be useful in making trade-off decisions in engineering
design problems.  \emph{[Refer to~\ref{capabilities:additional} for
additional information on multiobjective optimization methods.]}

\textbf{Mixed Integer Nonlinear Programming (MINLP)}: This strategy uses
the branch and bound capabilities of the PICO package to perform
optimization on problems that have both discrete and continuous design
variables. PICO provides a branch and bound engine targeted at mixed
integer linear programs (MILP), which when combined with DAKOTA's
nonlinear optimization methods, results in a MINLP capability. In
addition, the multiple NLPs solved within MINLP provide an opportunity
for concurrent execution of multiple optimizations.
\emph{For DAKOTA 5.1, branch and bound is currently inoperative due to 
ongoing restructuring of PICO and its incorporation into COLINY.
This will be supported again in future releases.}

These strategies are covered in more detail in Chapter~\ref{strat}.

\section{Surrogate Models}\label{capabilities:surrogate}

Surrogate models are inexpensive approximate models that are intended
to capture the salient features of an expensive high-fidelity model.
They can be used to explore the variations in response quantities over
regions of the parameter space, or they can serve as inexpensive
stand-ins for optimization or uncertainty quantification studies (see,
for example, the surrogate-based optimization strategy in
Section~\ref{capabilities:optimization2}).  The surrogate models
supported in DAKOTA can be categorized into three types: data fits,
multifidelity, and reduced-order model surrogates.

Data fitting methods involve construction of an approximation or
surrogate model using data (response values, gradients, and Hessians)
generated from the original truth model.  Data fit methods can be
further categorized as local, multipoint, and global approximation
techniques, based on the number of points used in generating the data
fit.  Local methods involve response data from a single point in
parameter space.  Available techniques currently include:

\textbf{Taylor Series Expansion}: This is a local first-order or
second-order expansion centered at a single point in the parameter space.

Multipoint approximations involve response data from two or more
points in parameter space, often involving the current and previous
iterates of a minimization algorithm.  Available techniques currently
include:

\textbf{TANA-3}: This multipoint approximation uses a two-point
exponential approximation~\cite{Xu98,Fad90} built with response value
and gradient information from the current and previous iterates.

Global methods, often referred to as \emph{response surface methods},
involve many points spread over the parameter ranges of interest.
These surface fitting methods work in conjunction with the sampling
methods and design of experiments methods described in
Section~\ref{capabilities:sampling}.

\textbf{Polynomial Regression}: First-order (linear), second-order
(quadratic), and third-order (cubic) polynomial response surfaces
computed using linear least squares regression methods. Note: there is
currently no use of forward- or backward-stepping regression methods
to eliminate unnecessary terms from the polynomial model.

\textbf{Gaussian Process (GP) or Kriging Interpolation}
DAKOTA contains two implementations of Gaussian process, also known as 
Kriging ~\cite{Giu98}, spatial interpolation.  One of these resides in 
the Surfpack sub-package of DAKOTA, the other resides in DAKOTA itself.
Both versions use the Gaussian correlation function with parameters that
are selected by Maximum Likelihood Estimation (MLE). This correlation 
function results in a response surface that is $C^\infty$-continuous.
Prior to DAKOTA 5.2, the Surfpack GP was referred to as the ``Kriging'' 
model and the DAKOTA version was labeled as the ``Gaussian Process.''  
These terms are now used interchangeably.  As of DAKOTA 5.2,the 
Surfpack GP is used by default.  For now the user still has the option 
to select the DAKOTA GP, but the DAKOTA GP is deprecated and will be 
removed in a future release.
\begin{itemize}
\item \textbf{Surfpack GP}: Ill-conditioning due to a poorly spaced sample 
      design is handled by discarding points that contribute the least 
      unique information to the correlation matrix.  Therefore, the points 
      that are discarded are the ones that are easiest to predict.  The 
      resulting surface will exactly interpolate the data values at the 
      retained points but is not guaranteed to interpolate the discarded 
      points.
\item \textbf{DAKOTA GP}: Ill-conditioning is handled by adding a jitter 
      term or ``nugget'' to diagonal elements of the correlation matrix. 
      When this happens, the DAKOTA GP may not exactly interpolate the 
      data values.
\end{itemize}

\textbf{Artificial Neural Networks}: An implementation of the
stochastic layered perceptron neural network developed by Prof. D. C.
Zimmerman of the University of Houston~\cite{Zim96}. This neural network
method is intended to have a lower training (fitting) cost than
typical back-propagation neural networks.

\textbf{Multivariate Adaptive Regression Splines (MARS)}: Software
developed by Prof. J. H. Friedman of Stanford University~\cite{Fri91}.
The MARS method creates a $C^2$-continuous patchwork of splines in the
parameter space.

\textbf{Radial Basis Functions (RBF)}:  Radial basis functions are 
functions whose value typically depends on the distance from a center point, 
called the centroid.  The surrogate model approximation is constructed
as the weighted sum of individual radial basis functions. 

\textbf{Moving Least Squares (MLS)}: Moving Least Squares can be 
considered a more specialized version of linear regression models.
MLS is a weighted least squares approach where the weighting is 
``moved'' or recalculated for every new point where 
a prediction is desired.~\cite{Nea04} 

%\textbf{Orthogonal Polynomials}: This technique involves the use of
%multivariate orthogonal polynomials as a global basis for surrogate
%modeling.  These multivariate polynomials are constructed as a product
%of particular univariate orthogonal polynomials, including Hermite,
%Legendre, Laguerre, Jacobi, and generalized Laguerre polynomials,
%which are defined as functions of standard normal, standard uniform,
%standard exponential, standard beta, and standard gamma random
%variables, respectively.  Given the probabilistic interpretation of
%the approximation variables, this data fit is primarily used for
%uncertainty quantification, and in particular, polynomial chaos
%expansions.

In addition to data fit surrogates, DAKOTA supports multifidelity 
and reduced-order model approximations:

\textbf{Multifidelity Surrogates}: Multifidelity modeling involves the
use of a low-fidelity physics-based model as a surrogate for the
original high-fidelity model.  The low-fidelity model typically
involves a coarser mesh, looser convergence tolerances, reduced
element order, or omitted physics.  It is a separate model in its own
right and does not require data from the high-fidelity model for
construction.  Rather, the primary need for high-fidelity evaluations
is for defining correction functions that are applied to the
low-fidelity results.

\textbf{Reduced Order Models}: A reduced-order model (ROM) is
mathematically derived from a high-fidelity model using the technique
of Galerkin projection.  By computing a set of basis functions (e.g.,
eigenmodes, left singular vectors) that capture the principal dynamics
of a system, the original high-order system can be projected to a much
smaller system, of the size of the number of retained basis functions.

Additional information on these surrogate methods is provided in
Sections~\ref{models:surrogate:datafit} through~\ref{models:surrogate:rom}.

\section{Nested Models}\label{capabilities:nested}
Nested models utilize a sub-iterator and a sub-model to perform a
complete iterative study as part of every evaluation of the model.
This sub-iteration accepts variables from the outer level, performs
the sub-level analysis, and computes a set of sub-level responses
which are passed back up to the outer level.  The nested model
constructs admit a wide variety of multi-iterator, multi-model
solution approaches.  For example, optimization within optimization
(for hierarchical multidisciplinary optimization), uncertainty
quantification within uncertainty quantification (for second-order
probability), uncertainty quantification within optimization (for
optimization under uncertainty), and optimization within uncertainty
quantification (for uncertainty of optima) are all supported, with and
without surrogate model indirection.  Three important examples are
highlighted: mixed epistemic-aleatory uncertainty quantification, 
optimization under uncertainty, and surrogate-based uncertainty quantification.

\textbf{Mixed Epistemic-Aleatory Uncertainty Quantification}: Mixed
uncertainty quantification (UQ) refers to capabilities for performing
UQ calculations on both epistemic uncertainties (also known as
reducible uncertainties resulting from a lack of knowledge) and
aleatory uncertainties (also known as irreducible uncertainties that
are inherent variabilities).  Mixed UQ approaches employ nested models
to embed one uncertainty quantification within another.  The outer
level UQ is commonly linked to epistemic uncertainties, and the inner
UQ is commonly linked to aleatory uncertainties.  We have three main
approaches: interval-valued probability, second-order probability, and
nested Dempster-Shafer.  In interval-valued probability, the outer
level generates sets of realizations, typically from sampling within
interval distributions.  These realizations define values for the
epistemic variables used in a probabilistic analysis for the inner
level UQ (e.g. which may involve sampling over aleatory variables).
In interval-valued probability, we generate intervals on statistics
from the inner loop.  In second-order probability, the outer level
also generates sets of realizations, from sampling from distributions
on the epistemic variables.  These outer loop values then define
values for distribution parameters used in the inner level UQ.  The
term ``second-order'' derives from this use of distributions on
distributions and the generation of statistics on statistics.  DAKOTA
includes capability to use interval analysis to perform the outer loop
calculations (e.g. find intervals on inner loop statistics).  Interval
analysis can use efficient optimization methods to obtain interval
bound estimates.  Nested Dempster-Shafer refers to using
Dempster-Shafer evidence theory on the outer loop, and an aleatory UQ
method such as sampling or stochastic expansions on the inner loop.
Evidence theory results in measures of belief and plausibility, so in a
nested context, this produces belief and plausibility bounds on inner
loop statistics.  More on mixed UQ approaches can be found
in~\cite{Eld09b} and ~\cite{Eld11}.

\textbf{Optimization Under Uncertainty (OUU)}: Many real-world
engineering design problems contain stochastic features and must be
treated using OUU methods such as robust design and reliability-based
design. For OUU, the uncertainty quantification methods of DAKOTA are
combined with optimization algorithms. This allows the user to
formulate problems where one or more of the objective and constraints
are stochastic. Due to the computational expense of both optimization
and UQ, the simple nesting of these methods in OUU can be
computationally prohibitive for real-world design problems. For this
reason, surrogate-based optimization under uncertainty (SBOUU),
reliability-based design optimization (RBDO), polynomial chaos-based
design optimization (PCBDO), and stochastic collocation-based design
optimization (SCBDO) methods have been developed which can reduce the
overall expense by orders of magnitude. OUU methods are an active
research area.

\textbf{Surrogate-Based Uncertainty Quantification (SBUQ)}: 
Since many uncertainty quantification (UQ) methods are computationally
costly, requiring many function evaluations to obtain accurate
estimates of moments or percentile values of an output distribution,
one may wish to embed surrogate models within the UQ process in order
to reduce expense.  By evaluating the true function on a fixed, small
set of samples and using these sample evaluations to create a response
surface approximation (e.g. a surrogate model or meta-model) of the
underlying ``true'' function, the subsequent evaluation of the UQ
results (using thousands or millions of samples) based on the
approximation can obtain estimates of the mean, variance, and
percentiles of the response at much lower overall cost.

Additional information on these nested approaches is provided in
Section~\ref{models:nested} and Chapter~\ref{adv_models}.

\section{Parallel Computing}\label{capabilities:parallel}

The methods and strategies in DAKOTA are designed to exploit parallel
computing resources such as those found in a desktop multiprocessor
workstation, a network of workstations, or a massively parallel
computing platform. This parallel computing capability is a critical
technology for rendering real-world engineering design problems
computationally tractable. DAKOTA employs the concept of
\emph{multilevel parallelism}, which takes simultaneous advantage of
opportunities for parallel execution from multiple sources:

\textbf{Parallel Simulation Codes}: DAKOTA works equally well with both
serial and parallel simulation codes.

\textbf{Concurrent Execution of Analyses within a Function Evaluation}:
Some engineering design applications call for the use of multiple
simulation code executions (different disciplinary codes, the same
code for different load cases or environments, etc.) in order to
evaluate a single response data set (e.g., objective functions and
constraints) for a single set of parameters. If these simulation code
executions are independent (or if coupling is enforced at a higher
level), DAKOTA can perform them in parallel.

\textbf{Concurrent Execution of Function Evaluations within an Iterator}:
With very few exceptions, the iterative algorithms described in
Section~\ref{capabilities:parameter} through
Section~\ref{capabilities:nonlinear} all provide opportunities for the
concurrent evaluation of response data sets for different parameter
sets. Whenever there exists a set of design point evaluations that are
independent, DAKOTA can perform them in parallel.

\textbf{Concurrent Execution of Iterators within a Strategy}: Some of
the DAKOTA strategies described in
Section~\ref{capabilities:optimization2} generate a sequence of
iterator subproblems. For example, the MINLP, Pareto-set, and
multi-start strategies generate sets of optimization subproblems, and
the optimization under uncertainty strategy generates sets of
uncertainty quantification subproblems. Whenever these subproblems are
independent, DAKOTA can perform them in parallel.

It is important to recognize that these four parallelism levels are
nested, in that a strategy can schedule and manage concurrent
iterators, each of which may manage concurrent function evaluations,
each of which may manage concurrent analyses, each of which may
execute on multiple processors. Additional information on parallel
computing with DAKOTA is provided in Chapter~\ref{parallel}.

\section{Summary}\label{capabilities:summary}

DAKOTA is both a production tool for engineering design and analysis
activities and a research tool for the development of new algorithms
in optimization, uncertainty quantification, and related areas.
Because of the extensible, object-oriented design of DAKOTA, it is
relatively easy to add new iterative algorithms, strategies,
simulation interfacing approaches, surface fitting methods, etc. In
addition, DAKOTA can serve as a rapid prototyping tool for algorithm
development. That is, by having a broad range of building blocks
available (i.e., parallel computing, surrogate models, simulation
interfaces, fundamental algorithms, etc.), new capabilities can be
assembled rapidly which leverage the previous software investments.
For additional discussion on framework extensibility, refer to the
DAKOTA Developers Manual~\cite{DevMan}.

The capabilities of DAKOTA have been used to solve engineering design
and optimization problems at Sandia Labs, at other Department of
Energy labs, and by our industrial and academic collaborators. Often,
this real-world experience has provided motivation for research into
new areas of optimization. The DAKOTA development team welcomes
feedback on the capabilities of this software toolkit, as well as
suggestions for new areas of research.
