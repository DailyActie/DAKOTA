Blurb::
Markov Chain Monte Carlo algorithms from the QUESO package
Description::
For the QUESO method, one can use an emulator in the MCMC sampling. This will 
greatly improve the speed, since the Monte Carlo Markov Chain will generate 
thousands of samples on the emulator instead of the real simulation code. 
However, in the case of fast running evaluations, we recommend the use of 
no emulator. An emulator may be specified with the keyword \c emulator, 
followed by a \c gaussian_process emulator, a \c pce emulator (polynomial chaos
expansion), or a \c sc emulator (stochastic collocation). For the 
\c gaussian_process emulator, the user must specify whether to use the 
\c surfpack or \c dakota version of the Gaussian process. 
The user can define the number of samples 
\c emulator_samples from which the emulator should be built. It is also 
possible to build the Gaussian process from points read in from the 
\c import_points_file and to export approximation-based sample evaluations 
using \c export_points_file. For \c pce or \c sc, the user can define a \c sparse_grid_level. 

In terms of the MCMC sampling, one can specify the following for the QUESO method: 
With the \c metropolis type, we have the options \c hastings for a standard 
Metropolis-Hastings algorithm, or \c adaptive for the adaptive Metropolis 
in which the covariance of the proposal density is updated adaptively. 
For the delayed rejection part of the DRAM algorithm, one specifies \c rejection, 
followed by \c standard (no delayed rejection) or \c delayed. Finally, the user 
has two scale factors which help control the scaling involved in the problem. 
The \c likelihood_scale is a number which scales the likelihood by dividing 
the log of the likelihood (e.g. dividing the sum of squared differences 
between the experimental data and simulation data or SSE). This 
is useful for situations with very small likelihoods (e.g. the model is either 
very far away from the data or there is a lot of data so the likelihood function 
involves multiplying many likelihoods together, where the SSE term is large 
and the likelihood becomes very small). 
In some respects, the \c likelihood_scale can be seen as a normalizing factor
for the SSE. If the SSE is large, the \c likelihood_scale should be large. 
The second factor is a \c proposal_covariance_scale which scales the proposal 
covariance. This may be useful when the input variables being calibrated 
are of different magnitudes: one may want to take a larger step in a direction 
with a larger magnitude, for example. Finally, we offer the option 
to calibrate the sigma terms with the \c calibrate_sigma flag. 
The sigma terms refer to the error associated 
with the Gaussian process: sigma is used in the likelihood calculation. 
If experimental measurement error is available to inform sigma, that is 
very useful, but often measurement uncertainty is not available. Note that 
if \c calibrate_sigma is specified, a separate sigma term will be calibrated 
for each calibration term. Thus, if there are 50 calibration terms (e.g. 
experimental points against which we are trying to match the model), 
50 sigma values will be added to the calibration process. Calibration 
of the sigma values is turned off by default: only the design parameters are 
calibrated in default mode. 


Topics::	bayesian_calibration, package_queso
Examples::
Theory::
Faq::
See_Also::	
