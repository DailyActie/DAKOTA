Blurb::
An empirical model that is created from data or the results of a submodel
Description::
Surrogate models are inexpensive approximate models that are intended
to capture the salient features of an expensive high-fidelity model.
They can be used to explore the variations in response quantities over
regions of the parameter space, or they can serve as inexpensive
stand-ins for optimization or uncertainty quantification studies (see,
for example, the surrogate-based optimization methods,
\ref method-surrogate_based_global and \ref method-surrogate_based_local).
Surrogate models are used extensively in the
surrogate-based optimization and least squares methods, in
which the goals are to reduce expense by minimizing the number of
truth function evaluations and to smooth out noisy data with a global
data fit. However, the use of surrogate models is not restricted to
optimization techniques; uncertainty quantification and optimization
under uncertainty methods are other primary users.

Surrogate models supported in Dakota can be
categorized into three types, with the listed approaches supported:

* Data fits: Data fitting methods involve construction of an
  approximation or surrogate model using data (response values,
  gradients, and Hessians) generated from the original truth model.
  Data fit methods can be further categorized as local, multipoint,
  and global approximation techniques, based on the number of points
  used in generating the data fit.
  * Local: built from response data from a single point in parameter space
    * Taylor series expansion
  * Multipoint:  built from two or more points in parameter space, often involving the current and previous iterates of a minimization algorithm.
    * TANA-3
  * Global full space response surface methods: 
    * Polynomial regression
    * Gaussian process (Kriging)
    * Artifical neutral network
    * MARS
    * RBF
    * Orthogonal polynomials (only supported in PCE/SC for now)
* Multifidelity: Multifidelity modeling involves the use of a
  low-fidelity physics-based model as a surrogate for the original
  high-fidelity model. The low-fidelity model typically involves a
  coarser mesh, looser convergence tolerances, reduced element order,
  or omitted physics.
* Reduced-order model surrogates: none currently active in Dakota <b>
  Reduced Order Models </b>: A reduced-order model (ROM) is
  mathematically derived from a high-fidelity model using the
  technique of Galerkin projection. By computing a set of basis
  functions (e.g., eigenmodes, left singular vectors) that capture
  the principal dynamics of a system, the original high-order system
  can be projected to a much smaller system, of the size of the
  number of retained basis functions.

Each of these surrogate types provides an approximate representation
of a "truth" model which is used to perform the parameter to response
mappings. This approximation is built and updated using data from the
truth model. This data is generated in some cases using a design of
experiments iterator applied to the truth model (global approximations
with a \c dace_method_pointer). In other cases, truth model data from
a single point (local, hierarchical approximations), from a few
previously evaluated points (multipoint approximations), or from the
restart database (global approximations with \c reuse_samples) can be
used. 

An overview and discussion of surrogate correction applicable to many
surrogate types in provided here in Theory, with details following in
the various surrogate types.

Topics::	not_yet_reviewed
Examples::
Theory::


<b> Correction Approaches </b>

Each of the surrogate model types supports the use of correction
factors that improve the local accuracy of the surrogate models. The
correction factors force the surrogate models to match the true
function values and possibly true function derivatives at the center
point of each trust region. Currently, Dakota supports either zeroth-,
first-, or second-order accurate correction methods, each of which can
be applied using either an additive, multiplicative, or combined
correction function. For each of these correction approaches, the
correction is applied to the surrogate model and the corrected model
is then interfaced with whatever algorithm is being employed. The
default behavior is that no correction factor is applied.

The simplest correction approaches are those that enforce consistency
in function values between the surrogate and original models at a
single point in parameter space through use of a simple scalar offset
or scaling applied to the surrogate model. First-order corrections
such as the first-order multiplicative correction (also known as beta
correction \ref Cha93) and the first-order additive
correction \ref Lewis2000 also enforce consistency in the gradients and
provide a much more substantial correction capability that is
sufficient for ensuring provable convergence in SBO algorithms.
SBO convergence rates can be further
accelerated through the use of second-order corrections which also
enforce consistency in the Hessians \ref Eldred2004a, where the
second-order information may involve analytic, finite-difference, or
quasi-Newton Hessians.

Correcting surrogate models with additive corrections involves
\anchor eq-correct_val_add
\f{equation}
\hat{f_{hi_{\alpha}}}({\bf x}) = f_{lo}({\bf x}) + \alpha({\bf x}) 
\f}
where multifidelity notation has been adopted for clarity. For
multiplicative approaches, corrections take the form
\anchor eq-correct_val_mult
\f{equation}
\hat{f_{hi_{\beta}}}({\bf x}) = f_{lo}({\bf x}) \beta({\bf x})
\f}
where, for local corrections, \f$\alpha({\bf x})\f$ and \f$\beta({\bf x})\f$
are first or second-order Taylor series approximations to the exact
correction functions:
\anchor eq-taylor_a
\anchor eq-taylor_b
\f{eqnarray}
\alpha({\bf x}) & = & A({\bf x_c}) + \nabla A({\bf x_c})^T 
({\bf x} - {\bf x_c}) + \frac{1}{2} ({\bf x} - {\bf x_c})^T 
\nabla^2 A({\bf x_c}) ({\bf x} - {\bf x_c})  \\
\beta({\bf x}) & = & B({\bf x_c}) + \nabla B({\bf x_c})^T 
({\bf x} - {\bf x_c}) + \frac{1}{2} ({\bf x} - {\bf x_c})^T \nabla^2 
B({\bf x_c}) ({\bf x} - {\bf x_c})
\f}
where the exact correction functions are
\anchor eq-exact_B
\anchor eq-exact_A 
\f{eqnarray}
A({\bf x}) & = & f_{hi}({\bf x}) - f_{lo}({\bf x})    \\
B({\bf x}) & = & \frac{f_{hi}({\bf x})}{f_{lo}({\bf x})} 
\f}
Refer to \ref Eldred2004a for additional details on the derivations.

A combination of additive and multiplicative corrections can provide
for additional flexibility in minimizing the impact of the correction
away from the trust region center. In other words, both additive and
multiplicative corrections can satisfy local consistency, but through
the combination, global accuracy can be addressed as well. This
involves a convex combination of the additive and multiplicative
corrections:
\f[ \hat{f_{hi_{\gamma}}}({\bf x}) = \gamma \hat{f_{hi_{\alpha}}}({\bf x}) +
(1 - \gamma) \hat{f_{hi_{\beta}}}({\bf x})  \f]
where \f$\gamma\f$ is calculated to satisfy an additional matching
condition, such as matching values at the previous design iterate.

<!---

It should be noted that in both first order correction methods, the
function \f$\hat{f}(x)\f$ matches the function value and gradients of
\f$f_{t}(x)\f$ at \f$x=x_{c}\f$. This property is necessary in proving that
the first order-corrected SBO algorithms are provably convergent to a
local minimum of \f$f_{t}(x)\f$. However, the first order correction
methods are significantly more expensive than the zeroth order
correction methods, since the first order methods require computing
both \f$\nabla f_{t}(x_{c})\f$ and \f$\nabla f_{s}(x_{c})\f$. When the SBO
strategy is used with either of the zeroth order correction methods,
or with no correction method, convergence is not guaranteed to a local
minimum of \f$f_{t}(x)\f$. That is, the SBO strategy becomes a heuristic
optimization algorithm. From a mathematical point of view this is
undesirable, but as a practical matter, the heuristic variants of SBO
are often effective in finding local minima.

\emph{Usage guidelines:}

\li Both the \c additive zeroth_order and
 \c multiplicative zeroth_order correction methods are
 "free" since they use values of \f$f_{t}(x_{c})\f$ that are normally
 computed by the SBO strategy.

\li The use of either the \c additive first_order method or
 the \c multiplicative first_order method does not necessarily
 improve the rate of convergence of the SBO algorithm.

\li When using the first order correction methods, the
 \c TRUE_FCN_GRAD response keywords must be modified (see
 bottom of Figure \ref{sbm:sblm_rosen}) to allow either analytic or
 numerical gradients to be computed. This provides the gradient data
 needed to compute the correction function.

\li For many computationally expensive engineering optimization
 problems, gradients often are too expensive to obtain or are
 discontinuous (or may not exist at all). In such cases the heuristic
 SBO algorithm has been an effective approach at identifying optimal
 designs \ref Giu02.

--->

<b>BMA: Not clear where to put this!  A lot could go to theory or users?</b>

<b> Data Fit Surrogate Models </b>

A surrogate of the {\em data fit} type is a non-physics-based
approximation typically involving interpolation or regression of a set
of data generated from the original model. Data fit surrogates can be
further characterized by the number of data points used in the fit,
where a local approximation (e.g., first or second-order Taylor
series) uses data from a single point, a multipoint approximation
(e.g., two-point exponential approximations (TPEA) or two-point
adaptive nonlinearity approximations (TANA)) uses a small number of
data points often drawn from the previous iterates of a particular
algorithm, and a global approximation (e.g., polynomial response
surfaces, kriging/gaussian_process, neural networks, radial basis 
functions, splines)
uses a set of data points distributed over the domain of interest,
often generated using a design of computer experiments.

Dakota contains several types of surface fitting methods that can be
used with optimization and uncertainty quantification methods and
strategies such as surrogate-based optimization and optimization under
uncertainty. These are: polynomial models (linear, quadratic, and
cubic), first-order Taylor series expansion, kriging spatial
interpolation, artificial neural networks, multivariate adaptive
regression splines, radial basis functions, and moving least squares. 
With the exception of Taylor series methods, all of the above methods 
listed in the previous sentence are accessed in Dakota through the 
Surfpack library. All of these surface fitting methods can be
applied to problems having an arbitrary number of design parameters.
However, surface fitting methods usually are practical only for
problems where there are a small number of parameters (e.g., a maximum
of somewhere in the range of 30-50 design parameters). The
mathematical models created by surface fitting methods have a variety
of names in the engineering community. These include surrogate models,
meta-models, approximation models, and response surfaces. For this
manual, the terms surface fit model and surrogate model are used.

The data fitting methods in Dakota include software developed by
Sandia researchers and by various researchers in the academic
community.

<b> Procedures for Surface Fitting </b>

The surface fitting process consists of three steps: (1) selection of
a set of design points, (2) evaluation of the true response quantities
(e.g., from a user-supplied simulation code) at these design points,
and (3) using the response data to solve for the unknown coefficients
(e.g., polynomial coefficients, neural network weights, kriging
correlation factors) in the surface fit model. In cases where there is
more than one response quantity (e.g., an objective function plus one
or more constraints), then a separate surface is built for each
response quantity. Currently, the surface fit models are built using
only 0\f$^{\mathrm{th}}\f$-order information (function values only), although
extensions to using higher-order information (gradients and Hessians)
are possible. Each surface fitting method employs a different
numerical method for computing its internal coefficients. For example,
the polynomial surface uses a least-squares approach that employs a
singular value decomposition to compute the polynomial coefficients,
whereas the kriging surface uses Maximum Likelihood Estimation to
compute its correlation coefficients. More information on the
numerical methods used in the surface fitting codes is provided in the
Dakota Developers Manual.

The set of design points that is used to construct a surface fit model
is generated using either the DDACE software package or the
LHS software package. These packages provide a variety of
sampling methods including Monte Carlo (random) sampling, Latin
hypercube sampling, orthogonal array sampling, central composite
design sampling, and Box-Behnken sampling.

<b> Taylor Series </b>

The Taylor series model is purely a local approximation method. That
is, it provides local trends in the vicinity of a single point in
parameter space. The first-order Taylor series expansion is:
\anchor eq-taylor1
\f{equation}
\hat{f}({\bf x}) \approx f({\bf x}_0) + \nabla_{\bf x} f({\bf x}_0)^T 
({\bf x} - {\bf x}_0) 
\f}
and the second-order expansion is:
\anchor eq-taylor2
\f{equation}
\hat{f}({\bf x}) \approx f({\bf x}_0) + \nabla_{\bf x} f({\bf x}_0)^T 
({\bf x} - {\bf x}_0) + \frac{1}{2} ({\bf x} - {\bf x}_0)^T 
\nabla^2_{\bf x} f({\bf x}_0) ({\bf x} - {\bf x}_0)
\f}

where \f${\bf x}_0\f$ is the expansion point in \f$n\f$-dimensional parameter
space and \f$f({\bf x}_0)\f$, \f$\nabla_{\bf x} f({\bf x}_0)\f$, and
\f$\nabla^2_{\bf x} f({\bf x}_0)\f$ are the computed response value,
gradient, and Hessian at the expansion point, respectively. As
dictated by the responses specification used in building the local
surrogate, the gradient may be analytic or numerical and the Hessian
may be analytic, numerical, or based on quasi-Newton secant updates.

In general, the Taylor series model is accurate only in the region of
parameter space that is close to \f${\bf x}_0\f$ . While the accuracy is
limited, the first-order Taylor series model reproduces the correct
value and gradient at the point \f$\mathbf{x}_{0}\f$, and the second-order
Taylor series model reproduces the correct value, gradient, and
Hessian. This consistency is useful in provably-convergent
surrogate-based optimization. The other surface fitting methods do not
use gradient information directly in their models, and these methods
rely on an external correction procedure in order to satisfy the
consistency requirements of provably-convergent SBO.






<b> Multifidelity Surrogate Models </b>

A second type of surrogate is the {\em model hierarchy} type (also
called multifidelity, variable fidelity, variable complexity, etc.).
In this case, a model that is still physics-based but is of lower
fidelity (e.g., coarser discretization, reduced element order, looser
convergence tolerances, omitted physics) is used as the surrogate in
place of the high-fidelity model. For example, an inviscid,
incompressible Euler CFD model on a coarse discretization could be
used as a low-fidelity surrogate for a high-fidelity Navier-Stokes
model on a fine discretization.

<b> Reduced Order Models </b>

A third type of surrogate model involves {\em reduced-order modeling}
techniques such as proper orthogonal decomposition (POD) in
computational fluid dynamics (also known as principal components
analysis or Karhunen-Loeve in other fields) or spectral decomposition
(also known as modal analysis) in structural dynamics. These
surrogate models are generated directly from a high-fidelity model
through the use of a reduced basis (e.g., eigenmodes for modal
analysis or left singular vectors for POD) and projection of the
original high-dimensional system down to a small number of generalized
coordinates. These surrogates are still physics-based (and may
therefore have better predictive qualities than data fits), but do not
require multiple system models of varying fidelity (as required for
model hierarchy surrogates).

<b> Surrogate Model Selection </b>

This section offers some guidance on choosing from among the available
surrogate model types.

\li For Surrogate Based Local Optimization, i.e. the 
   \c surrogate_based_local method, with a trust region, either
   \c surrogate \c local \c taylor_series or
   \c surrogate \c multipoint \c tana will probably 
   work best. If for some reason you wish or need to use a global 
   surrogate (not recommended) then the best of these options is likely 
   to be either 
   \c surrogate \c global 
   \c gaussian_process \c surfpack or
   \c surrogate \c global \c moving_least_squares.
\li For Efficient Global Optimization (EGO), i.e. the 
   \c efficient_global method, the default\\
   \c gaussian_process \c surfpack 
   is likely to find a more optimal value and/or use fewer true 
   function evaluations than the alternative,
   \c gaussian_process \c dakota. However, the 
   \c surfpack version will likely take more time to build 
   than the \c dakota version. Note that currently the 
   \c use_derivatives keyword is not recommended for use with
   EGO based methods.
\li For EGO based global interval estimation (EGIE), i.e. the 
   \c global_interval_est \c ego method, 
   the default \c gaussian_process \c surfpack will
   likely work better than the alternative \c gaussian_process 
   \c dakota.
\li For Efficient Global Reliability Analysis (EGRA), i.e. the 
   \c global_reliability method the \c surfpack and 
   \c dakota versions of the gaussian process tend to give 
   similar answers with the \c dakota version tending to use
   fewer true function evaluations. Since this is based on EGO, it
   is likely that the default \c surfpack version is more 
   accurate, although this has not been rigorously demonstrated.
\li For EGO based Dempster-Shafer Theory of Evidence, i.e. the 
   \c global_evidence \c ego method, the default
   \c gaussian_process \c surfpack will often use
   significantly fewer true function evaluations than the 
   alternative \c gaussian_process \c dakota.
\li When using a global surrogate to extrapolate, either the
   \c gaussian_process \c surfpack or 
   \c polynomial \c quadratic or 
   \c polynomial \c cubic is recommended.
\li When there is over roughly two or three thousand data points 
   and you wish to interpolate (or approximately interpolate) then 
   a Taylor series, Radial Basis Function Network, or Moving Least
   Squares fit is recommended. The only reason that the 
   \c gaussian_process \c surfpack model is not 
   recommended is that it can take a considerable amount of time
   to construct when the number of data points is very large. Use 
   of the third party MARS package included in Dakota is generally 
   discouraged.
\li In other situations that call for a global surrogate, the 
   \c gaussian_process \c surfpack is generally 
   recommended. The \c use_derivatives keyword will 
   only be useful if accurate and an inexpensive derivatives 
   are available. Finite difference derivatives are disqualified 
   on both counts. However, derivatives generated by analytical,
   automatic differentiation, or continuous adjoint techniques
   can be appropriate. Currently, first order derivatives, i.e.
   gradients, are the highest order derivatives that can be used
   to construct the \c gaussian_process \c surfpack
   model; Hessians will not be used even if they are available.

Faq::
See_Also::	model-single, model-nested
