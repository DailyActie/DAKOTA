Blurb::
Bayesian calibration, using the QUESO package
Description::
Currently, we are in the process of incorporating Bayesian calibration 
methods in %Dakota. These methods take prior information on parameter 
values (in the form of prior distributions) and observational data 
(e.g. from experiments) and produce posterior distributions on the 
parameter values. When the computational simulation is then executed
with samples from the posterior parameter distributions, the 
results that are produced are consistent with ("agree with") the 
experimental data. In the case of calibrating parameters from a 
computational simulation model, we require a "likelihood function" that 
specifies the likelihood of observing a particular observation given 
the model and its associated parameterization. We assume a Gaussian 
likelihood function currently. The algorithms that produce 
the posterior distributions on model parameters are Monte Carlo Markov 
Chain (MCMC) sampling algorithms. MCMC methods require many samples, often 
tens of thousands, so in the case of model calibration, often emulators 
of the computational simulation are used. For more details on the algorithms
underlying the methods, see the %Dakota User's manual. 

We have two Bayesian calibration methods under development in %Dakota: 
one called QUESO and one called GPMSA. They are specified with the
\c bayes_calibration \c queso or \c bayes_calibration \c gpmsa, respectively. 
The QUESO method is using components from the QUESO library 
(Quantification of Uncertainty for Estimation, Simulation, and 
Optimization) developed at The University of Texas at Austin. 
We are using a DRAM (Delayed Rejected Adaptive Metropolis) algorithm 
for the MCMC sampling from the QUESO library. 
GPMSA (Gaussian Process Models for Simulation Analysis) is 
a code that has been developed at Los Alamos National Laboratory. 
It uses Gaussian process models as part of constructing 
an emulator for the expensive 
computational simulation. GPMSA also has extensive features for 
calibration, such as the capability to include a "model discrepancy" term
and the capability to model functional data such as time series data. 
 
For the QUESO method, one can use an emulator in the MCMC sampling. This will 
greatly improve the speed, since the Monte Carlo Markov Chain will generate 
thousands of samples on the emulator instead of the real simulation code. 
However, in the case of fast running evaluations, we recommend the use of 
no emulator. An emulator may be specified with the keyword \c emulator, 
followed by a \c gaussian_process emulator, a \c pce emulator (polynomial chaos
expansion), or a \c sc emulator (stochastic collocation). For the 
\c gaussian_process emulator, the user must specify whether to use the 
\c surfpack or \c dakota version of the Gaussian process. 
The user can define the number of samples 
\c emulator_samples from which the emulator should be built. It is also 
possible to build the Gaussian process from points read in from the 
\c points_file. For \c pce or \c sc, the user can define a \c sparse_grid_level. 

In terms of the MCMC sampling, one can specify the following for the QUESO method: 
With the \c metropolis type, we have the options \c hastings for a standard 
Metropolis-Hastings algorithm, or \c adaptive for the adaptive Metropolis 
in which the covariance of the proposal density is updated adaptively. 
For the delayed rejection part of the DRAM algorithm, one specifies \c rejection, 
followed by \c standard (no delayed rejection) or \c delayed. Finally, the user 
has two scale factors which help control the scaling involved in the problem. 
The \c likelihood_scale is a number which multiplies the likelihood. This 
is useful for situations with very small likelihoods (e.g. the model is either 
very far away from the data or there is a lot of data so the likelihood function 
involves multiplying many likelihoods together and becomes very small). 
The second factor is a \c proposal_covariance_scale which scales the proposal 
covariance. This may be useful when the input variables being calibrated 
are of different magnitudes: one may want to take a larger step in a direction 
with a larger magnitude, for example. Finally, we offer the option 
to calibrate the sigma terms with the \c calibrate_sigma flag. 
The sigma terms refer to the error associated 
with the Gaussian process: sigma is used in the likelihood calculation. 
If experimental measurement error is available to inform sigma, that is 
very useful, but often measurement uncertainty is not available. Note that 
if \c calibrate_sigma is specified, a separate sigma term will be calibrated 
for each calibration term. Thus, if there are 50 calibration terms (e.g. 
experimental points against which we are trying to match the model), 
50 sigma values will be added to the calibration process. Calibration 
of the sigma values is turned off by default: only the design parameters are 
calibrated in default mode. 

For the GPMSA method, one can define the number of samples which will be used 
in construction of the emulator, \c emulator_samples. The emulator 
involves Gaussian processes in GPMSA, so the user does not specify anything 
about emulator type. At this point, the only controls active for GPMSA 
are \c emulator_samples, \c seed and \c rng, and \c samples (the number of MCMC
samples). 

As mentioned above, the Bayesian capability in %Dakota currently relies 
on the QUESO library developed by The University of Texas at Austin. 
This integrated capability is still in prototype form and available to 
close collaborators of the %Dakota team.
If you are interested in this capability, contact the %Dakota developers 
at dakota-developers@development.sandia.gov.


Topics::	package_queso, not_yet_reviewed
Examples::
Theory::
Faq::
See_Also::	
