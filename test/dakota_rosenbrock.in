# DAKOTA INPUT FILE - dakota_rosenbrock.in

# This sample Dakota input file optimizes the Rosenbrock function.
# See p. 95 in Practical Optimization by Gill, Murray, and Wright.

# This tester fills out a test matrix of CG/QNewton/FDNewton/Newton/
# GNewton/PDS vs. vendor numerical/dakota analytic gradients with
# bound constraints applied where supported.  NLSSOL is also added
# in for least squares coverage.

method,
	optpp_q_newton			#0,#1
#	optpp_fd_newton			#2,#3
#	optpp_cg		   	#4,#5
#	optpp_newton			#6
#	optpp_g_newton			#7
#	optpp_pds			#8
#	nlssol_sqp			#9,#10
#	nl2sol				#11,#12,#13,#14
	  max_iterations = 50           #0,#1,#2,#3,#6,#7,#8,#9,#10,#11,#12,#13,#14
	  convergence_tolerance = 1e-4  #0,#1,#2,#3,#6,#7,#8,#9,#10,#11,#12,#13,#14
          # CG is a poor performer on Rosenbrock      
#	  max_iterations = 100          #4,#5
#	  convergence_tolerance = 1e-6  #4,#5
#	  output debug

variables,
	continuous_design = 2
	  initial_point  -1.2  1.0
	  lower_bounds   -2.0 -2.0	#0,#1,#2,#3,#6,#7,#8,#9,#10,#11,#12
	  upper_bounds    2.0  2.0	#0,#1,#2,#3,#6,#7,#8,#9,#10,#11,#12
	  descriptors     'x1' 'x2'

interface,
	system
	  analysis_driver = 'rosenbrock'

responses,
	num_objective_functions = 1	#0,#1,#2,#3,#4,#5,#6,#8
#	num_least_squares_terms = 2	#7,#9,#10,#11,#12,#13,#14
#	no_gradients			#8
	analytic_gradients		#0,#2,#4,#6,#7,#9,#11,#13
#	numerical_gradients		#1,#3,#5,#10,#12,#14
#	  method_source vendor		#1,#3,#5,#10,#12,#14
#	  interval_type forward		#1,#3,#5,#10
#	  fd_gradient_step_size = 1.e-5	#1,#3,#5,#10
	no_hessians		#0,#1,#2,#3,#4,#5,#7,#8,#9,#10,#11,#12,#13,#14
#	analytic_hessians		#6
